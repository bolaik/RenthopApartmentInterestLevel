list.files()
library(tm)
install.packages('tm')
library(tm)
txt <- system.file("texts", "pressRelease.txt", package = "tm")
ovid <- VCorpus(DirSource(txt, encoding = "UTF-8"), readerControl = list(language = "en"))
(ovid <- VCorpus(DirSource(txt, encoding = "UTF-8"), readerControl = list(language = "en")))
txt <- system.file("texts", "/", package = "tm")
txt <- system.file("texts", "~/", package = "tm")
ovid <- VCorpus(DirSource(txt, encoding = "UTF-8"), readerControl = list(language = "en"))
txt <- system.file("texts", "", package = "tm")
txt <- system.file("texts", "txt", package = "tm")
txt <- system.file("texts", "txt", package = "tm")
ovid <- VCorpus(DirSource(txt, encoding = "UTF-8"), readerControl = list(language = "en"))
inspect(ovid[1:2])
writeLines(as.character(ovid[[2]]))
docs <- c("This is a text.", "This another one.")
VCorpus(VectorSource(docs))
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
txt <- system.file("texts", "txt", package = "tm")
ovid <- Corpus(DirSource(txt, encoding = "UTF-8"),
+ readerControl = list(language = "lat"))
ovid <- Corpus(DirSource(txt, encoding = "UTF-8"), readerControl = list(language = "lat"))
writeCorpus(ovid)
setwd("~/")
inspect(ovid[1:2])
ovid_1.txt
meta(ovid[[2]], "id")
identical(ovid[[2]], ovid[["ovid_2.txt"]])
docs <- c("This is a text.", "This another one.")
VCorpus(VectorSource(docs))
txt <- system.file("texts", "txt", package = "tm")
ovid <- Corpus(DirSource(txt, encoding = "UTF-8"), readerControl = list(language = "lat"))
writeLines(as.character(ovid[[2]]))
lapply(ovid[1:2], as.character)
inspect(reuters)
meta(reuters[[1]],"id")
meta(reuters[[2]],"id")
meta(reuters[[20]],"id")
meta(reuters,"id")
meta(reuters[[:]],"id")
meta(reuters[[]],"id")
writeLines(as.character(reuters[[2]]))
reuters <- tm_map(reuters, stripWhitespace)
reuters <- tm_map(reuters, content_transformer(tolower))
reuters <- tm_map(reuters, removeWords, stopwords("english"))
tm_map(reuters, stemDocument)
idx <- meta(reuters, "id") == '237' & meta(reuters, "heading") == 'INDONESIA SEEN AT CROSSROADS OVER ECONOMIC CHANGE'
reuters[idx]
idx <- meta(reuters, "id") == '237'
reuters[idx]
DublinCore(crude[[1]], "Creator") <- "Ano Nymous"
meta(crude[[1]])
DublinCore(crude[[1]], "Creator") <- "Wei Xu"
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- Corpus(DirSource(reut21578),  readerControl = list(reader = readReut21578XML))
DublinCore(crude[[1]], "Creator") <- "Wei Xu"
inspect(reuters)
writeLines(reuters[[20]])
writeLines(as.character(reuters[[20]]))
writeLines(as.character(reuters[[1]]))
q()
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- Corpus(DirSource(reut21578),  readerControl = list(reader = readReut21578XML))
library(tm)
reuters <- Corpus(DirSource(reut21578),  readerControl = list(reader = readReut21578XML))
writeLines(as.character(reuters[[1]]))
install.packages("tm")
q()
install.packages("tm")
library(tm)
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- Corpus(DirSource(reut21578),  readerControl = list(reader = readReut21578XML))
writeLines(as.character(reuters[[1]]))
writeLines(as.character(reuters[[2]]))
reuters <- tm_map(reuters, stripWhitespace)
q()
library(tm)
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- Corpus(DirSource(reut21578),  readerControl = list(reader = readReut21578XML))
reuters <- tm_map(reuters, as.PlainTextDocument)
reuters <- tm_map(reuters, stripWhitespace)
reuters <- tm_map(reuters, tolower)
reuters <- tm_map(reuters, removeWords, stopwords("english"))
tm_map(reuters, stemDocument)
writeLines(as.character(reuters[[1]]))
inspect(reuters)
q()
knit_with_parameters('~/R/JHU/Data Science Capstone/nlp-wei/nlp-eda/nlp-eda.Rmd')
install.packages("vignette")
q()
txt <- "i would like to stay in the middle of 1990s and never become the 1st in 2000s."
library(tm)
mycorpus <- VCorpus(VectorSource(txt))
writeLines(as.character(mycorpus[[1]]))
tm_map(mycorpus, removeNumbers)
rnum <- tm_map(mycorpus, removeNumbers)
writeLines(as.character(rnum[[1]]))
rstopword <- tm_map(mycorpus, removeWords, stopwords("english"))
writeLines(as.character(rstopword[[1]]))
rstopword1 <- tm_map(rstopword, stripWhitespace)
writeLines(as.character(rstopword1[[1]]))
rstopword1 <- tm_map(rstopword, removePunctuation)
writeLines(as.character(rstopword1[[1]]))
q()
grepl("[[:space:][:digit:]]+", c("aa aa", "aa  aa"))
grepl("[[:space:]]+", c("aa aa", "aa  aa"))
grepl("[abc[:digit:]]+", c("aa aa", "dd  ee"))
grepl("[abc[:digit:]]+", c("aac", "ddee"))
grepl("[abc[:digit:]]+", c("aac", "ddeea"))
grepl("[abc[:digit:]]+", c("aac", "ddeeb"))
grepl("[abc[:digit:]]+", c("aac", "ddeec"))
?unlist
strsplit("aa  bb cc", split = "[[:space:][:digit:]]")
strsplit("aa  bb cc", split = "[[:space:][:digit:]]+")
strsplit("aa  bb cc", split = "[[:space:]]+")
strsplit("aa  bb cc", split = "[ ]+")
strsplit("aa  bb cc", split = "[ ]")
strsplit("aa  bb cc", split = "[ ]+")
trunc(123456/10000)
123456/10000
123456//10000
123456%/%10000
123456 %/% 10000
123456 %% 10000
q()
seq(1,46,5)
library(tm)
?writeCorpus
?PCorpus
docs <- c("This is a text.", "This is another one.")
text <- VCorpus(VectorSource(docs))
summary(text)
library(tau)
ngram <- textcnt(text, method = "string", n=2, split = "[]+", decreasing = TRUE)
ngram <- textcnt(text, method = "string", n=2, split = "[ ]+", decreasing = TRUE)
docs <- c("This is a text.", "This is another one.")
text <- VCorpus(VectorSource(docs))
ngram <- textcnt(text, method = "string", n=2, split = "[]+", decreasing = TRUE)
ngram <- textcnt(text, method = "string", n=2, split = "[[:space:]]+", decreasing = TRUE)
library(tm)
library(tau)
docs <- c("This is a text.", "This is another one.")
text <- VCorpus(VectorSource(docs))
ngram <- textcnt(text, method = "string", n=2, split = "[[:space:]]+", decreasing = TRUE)
ngram <- textcnt(text, method = "string", n=2, split = "[[:space:]]+", decreasing = TRUE)
writeLines(text[[1]])
writeLines(text[[1]][[1]])
writeLines(text[[2]][[1]])
ngram <- textcnt(text[[1]], method = "string", n=2, split = "[[:space:]]+", decreasing = TRUE)
ngram <- textcnt(text[[1]][[1]], method = "string", n=2, split = "[[:space:]]+", decreasing = TRUE)
library("tm")
data("crude")
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(removeSparseTerms(tdm[, 1:10], 0.7))
txt <- "this is a very good example, which should give your 1st time experience."
gsub(" [0-9]+(.*) " , " <num> ", txt)
txt <- "this is a very 9:30 example, which 12's give 13 1st time experience."
gsub(" [0-9]+(.*) " , " <num> ", txt)
gsub(" [0-9]+(.*\ ) " , " <num> ", txt)
gsub(" [0-9]+(.*\[ ]) " , " <num> ", txt)
gsub(" [0-9]+[^ ] " , " <num> ", txt)
txt
gsub(" [0-9]+[^ ]+" , " <num> ", txt)
gsub(" [0-9]+[^ ]+ " , " <num> ", txt)
gsub(" [0-9]*[^ ]+ " , " <num> ", txt)
gsub(" [0-9]+[^ ]+ " , " <num> ", txt)
txt
gsub(" [0-9]+[^ ]+ " , " <num> ", txt)
gsub(" [0-9]+[^ ]* " , " <num> ", txt)
gsub(" [0-9]+[^ ]* " , " <num> ", txt)
gsub(" [0-9]+[^ ]+ " , " <num> ", txt)
gsub(" [^ ]+[0-9]+[^ ]+ " , " <num> ", txt)
gsub(" [^ ]*[0-9]+[^ ]* " , " <num> ", txt)
gsub(" [^\\ ]*[0-9]+[^\\ ]* " , " <num> ", txt)
gsub(" [^\\ ]*[0-9][^\\ ]* " , " <num> ", txt)
gsub(" [^\\ ]*[0-9]+[^\\ ]* " , " <num> ", txt)
txt <- "this 11 12 very 9:30 example, which 12's give 13 and 1st time experience."
gsub(" [^\\ ]*[0-9]+[^\\ ]* " , " <num> ", txt)
gsub("[^\\ ]*[0-9]+[^\\ ]*" , "<num>", txt)
gsub("[^ ]*[0-9]+[^ ]*" , "<num>", txt)
txt <- "this is very "good example", which 12's && give 13 + and 1st :) time experience."
txt <- "this is very 'good example', which 12's && give 13 + and 1st :) time experience."
docs <- VCorpus(VectorSource(txt))
tm_map(docs, removePunctuation)
writeLines(tm_map(docs, removePunctuation)[[1]][[1]])
gsub( "[^[:alnum:],]", "", "asdfasdf,aswer?123'")
gsub( "[^[:alnum:],]", "", "asd f as df,asw er? 123'")
gsub( "[^[:alnum:],[ ]]", "", "asd f as df,asw er? 123'")
gsub( "[^[:alnum:]'.,?![ ]]", "", "as!d f' as df,asw er? 123'")
gsub( "[^[:alnum:]'.,?![ ]]", "", "as!d f' as df,asw er? 1)23'")
gsub( "[^[:alnum:]',.?\s]", "", "as!d f' as df,asw er? 1)23'")
gsub( "[^[:alnum:]',.?\\s]", "", "as!d f' as df,asw er? 1)23'")
gsub( "[^[:alnum:]',.?[:space:]]", "", "as!d f' as df,asw er? 1)23'")
gsub( "[^[:alnum:]',.?[:space:]]", "", "as!d f' as d-f,asw er? 1)23'")
gsub( "[^[:alnum:]',.?[\s]]", "", "as!d f' as d-f,asw er? 1)23'")
gsub( "[^[:alnum:]',.?[:space:]-]", "", "as!d f' as d-f,asw er? 1)23'")
gsub( "[^[:alnum:]',.?[:space:]-]", "", "as!d f' as d-f,asw er? 1)23 ab... is not!!! '")
gsub( "[^[:alnum:]',.?!:[:space:]-]", "", "as!d f' as d-f,asw er? 1)23 ab... is not!!! '")
gsub( "[^[:alnum:]\',.?!:[:space:]-]", "", "as!d f' as d-f,asw er? 1)23 ab... is not!!! '")
gsub( "[^[:alnum:]\',.?!:[:space:]-]", "", "as!d f'  ' as d-f,asw er? 1)23 ab... is not!!! '")
gsub( "[^[:alnum:]',.?!:[:space:]-]", "", "as!d f'  ' as d-f,asw er? 1)23 ab... is not!!! '")
gsub("http:[[:alnum:]]*", "", "https://rpubs.com/brianzive/textmining")
gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", "", "https://rpubs.com/brianzive/textmining")
gsub(" ?(f|ht)tp(s?):[[:alnum:]]*", "", "https://rpubs.com/brianzive/textmining")
gsub(" ?(f|ht)tp(s?):(.*)", "", "https://rpubs.com/brianzive/textmining")
gsub(" ?(f|ht)tp(s?):(.*)", "", "this is great:https://rpubs.com/brianzive/textmining.")
gsub(" ?(f|ht)tp(s?):(.*)", "", "this is great:https://rpubs.com/brianzive/textmining  ")
gsub(" ?(f|ht)tp(s?):[^ ]*", "", "this is great:https://rpubs.com/brianzive/textmining  ")
gsub(" ?(f|ht)tp(s?):[^ ]*", "", "this is great: https://rpubs.com/brianzive/textmining  ")
gsub(" ?(f|ht)tp(s?):[^ ]*", "", "this is great: https://rpubs.com/brianzive/textmining")
gsub(".[.]+", ".", "I hate you...")
gsub("\.[\.]+", "\.", "I hate you...")
gsub(".[.]+", ".", "I hate you...")
gsub("\.[.]+", ".", "I hate you...")
gsub(".[.]+", ".", "I hate you...")
gsub("\\.[\\.]+", "\\.", "I hate you...")
gsub( "[^[:alnum:]\\',\\.?!:[:space:]-]", "", "as!d f'  ' as d-f,asw er? 1)23 ab... is not!!! '")
gsub("!", ".", "I hate! you...")
gsub("\\/", " or ", "I hate/ you...")
gsub("\\/", " or ", "I hate/fuck you...")
gsub("\\&", " or ", "I hate&like you...")
gsub("\\&", " and ", "I hate&like you...")
gsub("[:punct:]", " [:punct:] ", "I hate&like you...")
gsub("[:punct:]", "", "I hate&like you...")
gsub("[[:punct:]]", "", "I hate&like you...")
gsub("[[:punct:]]", " [[:punct:]] ", "I hate&like you...")
gsub("[^ ]+@[^ ]+\\.[^ ]+", "<email>", "is this an email: xuweids1989@gmail.com")
gsub("[^ ]+@[^ ]+\\.[^ ]+", "<email>", "is this an email: xuweids1989@gmai")
gsub("[^ ]+@[^ ]+\\.[^ ]+", "<email>", "is this an email: xuweids1989@gmail.edu")
gsub("[^ ]+@[^ ]+\\.[^ ]+", "<email>", "is this an email: xuweids1989@gmail.edu.com")
gsub("[^ ]+@[^ ]+\\.[^ ]+", "<email>", "is this an email: (xuweids1989@gmail.edu.com")
gsub("[^ ]+@[^ ]+\\.[^ ]+", "<email>", "is this an email: (xuweids1989@gmail.edu.com"))
gsub("[^ ]+@[^ ]+\\.[^ ]+", "<email>", "is this an email: (xuweids1989@gmail.edu.com)")
gsub("<|>", "", "this <is> the so >< called> big.")
q()
install.packages("neuralnet")
library(neuralnet)
dim(infert)
head(infert)
nn <- neuralnet(case ~ age+parity+induced+spntaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn <- neuralnet(case ~ age+parity+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
?neuralnet
nn
plot(nn)
nn <- neuralnet(case ~ age+parity+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
plot(nn)
nn$net.result
nn$weights
plot(nn)
nn
plot(nn)
nn$result.matrix
plot(nn)
nn$net.result[[1]]
nn1 <- ifelse(nn$net.result[[1]]>0.5, 1.0)
nn1 <- ifelse(nn$net.result[[1]]>0.5, 1,0)
nn1
misClasificationError <- mean(infert$case != nn1)
misClasificationError
?neuralnet
nn <- neuralnet(case ~ age+parity+induced+spontaneous, data = infert, hidden = 2, learningrate = 0.01, algorithm = "backprop", err.fct = "ce", linear.output = FALSE)
nn.bp <- neuralnet(case ~ age+parity+induced+spontaneous, data = infert, hidden = 2, learningrate = 0.01, algorithm = "backprop", err.fct = "ce", linear.output = FALSE)
nn <- neuralnet(case ~ age+parity+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn <- neuralnet(case ~ age+parity+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn.bp <- neuralnet(case ~ age+parity+induced+spontaneous, data = infert, hidden = 2, learningrate = 0.01, algorithm = "backprop", err.fct = "ce", linear.output = FALSE)
nn.bp
summary(nn.bp)
print(nn.bp)
plot(nn.bp)
plot(nn)
new.output <- compute(nn, covariate = matrix(c(22,1,0,0,22,1,1,0,22,1,0,1,22,1,1,1), byrow = TRUE, ncol = 4))
new.output$net.result
ci <- confidence.interval(nn, alpha = 0.05)
ci
par(mfrow=c(2,2))
gwplot(nn, selected.covariate = "age", min = -2.5, max = 5)
gwplot(nn, selected.covariate = "parity", min = -2.5, max = 5)
gwplot(nn, selected.covariate = "induced", min = -2.5, max = 5)
gwplot(nn, selected.covariate = "spontaneous", min = -2.5, max = 5)
q()
devtools::install_github("stnava/ANTsR")
q()
install.packages("Rcpp")
devtools::install_github("stnava/ANTsR")
install.packages("drat")
drat::addRepo("ANTs-R")
install.packages("ANTsR")
q()
update.packages()
q()
update.packages()
q()
setwd("~/Programs/R/Kaggle/Twosigma-Rental-Listing/nyc-map")
install.packages("prettydoc")
geocode("new york", source = "google")
library(ggmap)
library(magrittr)
geocode("new york", source = "google")
class(geocode("new york", source = "google"))
q()
