{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Fold XGB CV\n",
    "\n",
    "### Problem of worse cv score with `json` file\n",
    "\n",
    "`basic_feat` imported from a pre-generated `json` file. The problem of worse cv score with `json` file has been resolved. And the reason is due to the fact that, in the underlying code:\n",
    "\n",
    "```python\n",
    "tr_sparse = vect_sparse[:train_num]\n",
    "te_sparse = vect_sparse[train_num:]\n",
    "```\n",
    "\n",
    "which requires in default the first `train_num` rows of `feat_all` should only contain training data. This is unfortunately not the case for the `json` file. So the following line is added to code:\n",
    "\n",
    "```python\n",
    "all_feat = all_feat.sort_values('interest_level', ascending=False).reset_index()\n",
    "```\n",
    "\n",
    "### Added custom function `most_freq_vects`\n",
    "\n",
    "Select most frequent terms in features, and the subsequent document-term-matrix\n",
    "\n",
    "### Pre-clean the features column data\n",
    "\n",
    "remove `[0-9]` and punctuations except `['|-]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_freq_vects(docs, max_feature=None, percent=None, token_pattern=u'(?u)\\b\\w\\w+\\b'):\n",
    "    vect = CountVectorizer(token_pattern=token_pattern)\n",
    "    feat_sparse = vect.fit_transform(docs.values.astype('U'))\n",
    "    freq_table = list(zip(vect.get_feature_names(), np.asarray(feat_sparse.sum(axis=0)).ravel()))\n",
    "    freq_table = pd.DataFrame(freq_table, columns=['feature', 'count']).sort_values('count', ascending=False)\n",
    "    if not max_feature:\n",
    "        if percent:\n",
    "            max_feature = int(percent * len(vect.get_feature_names()))\n",
    "        else:\n",
    "            max_feature = len(vect.get_feature_names())\n",
    "    feat_df = pd.DataFrame(feat_sparse.todense(), columns=vect.get_feature_names())\n",
    "    names = list(freq_table.feature[:max_feature])\n",
    "    return names, csr_matrix(feat_df[names].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_data_prep():\n",
    "    basic_feat = pd.read_json('feat_input/basic_feat.json')\n",
    "    longtime_feat = pd.read_csv('feat_input/longtime_feat.csv')\n",
    "    encoded_feat = pd.read_csv('feat_input/feat_stats_encoding.csv')\n",
    "    print('Loading features finished')\n",
    "\n",
    "    # apply ordinal encoding to categorical feature\n",
    "    basic_feat.display_address = basic_feat.display_address.replace(r'\\r$', '', regex=True)\n",
    "    basic_feat.street_address = basic_feat.street_address.replace(r'\\r$', '', regex=True)\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "    for f in categorical:\n",
    "        if basic_feat[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(basic_feat[f].values))\n",
    "            basic_feat[f] = lbl.transform(list(basic_feat[f].values))\n",
    "    print('Ordinal encoding finished')\n",
    "\n",
    "    all_feat = basic_feat.merge(longtime_feat, on='listing_id')\n",
    "    all_feat = all_feat.merge(encoded_feat, on='listing_id')\n",
    "\n",
    "    all_feat = all_feat.sort_values('interest_level', ascending=False).reset_index()\n",
    "    train = all_feat[all_feat.interest_level != -1].copy()\n",
    "    test = all_feat[all_feat.interest_level == -1].copy()\n",
    "    y_train=train[\"interest_level\"]\n",
    "\n",
    "    train_num=train.shape[0]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    import string\n",
    "    punct = string.punctuation\n",
    "    punct = re.sub(\"'|-\", \"\", punct)\n",
    "    pattern = r\"[0-9]|[{}]\".format(punct)\n",
    "    \n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [re.sub(pattern, \"\", y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ['_'.join(['feature'] + y.split()) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    vect_names, vect_sparse = most_freq_vects(all_feat['features'], max_feature=100, token_pattern=r\"[^ ]+\")\n",
    "    print(vect_names)\n",
    "    \n",
    "    tr_sparse = vect_sparse[:train_num]\n",
    "    te_sparse = vect_sparse[train_num:]\n",
    "    print(\"Document-term matrix finished\")\n",
    "\n",
    "    x_train = train.drop([\"interest_level\",\"features\"],axis=1)\n",
    "    x_test = test.drop([\"interest_level\",\"features\"],axis=1)\n",
    "\n",
    "    x_train = sparse.hstack([x_train.astype(float),tr_sparse.astype(float)]).tocsr()\n",
    "    x_test = sparse.hstack([x_test.astype(float),te_sparse.astype(float)]).tocsr()\n",
    "\n",
    "    return x_train, y_train, x_test, test.listing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv(dtrain, num_rounds = 50000, early_stop_rounds=250):\n",
    "    print('Start xgboost cross-validation')\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'gamma': 1,\n",
    "              'min_child_weight': 1.5,\n",
    "              'max_depth': 5,\n",
    "              'lambda': 10,\n",
    "              'subsample': 0.7,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'colsample_bylevel': 0.7,\n",
    "              'eta': 0.03,\n",
    "              'tree_method': 'exact',\n",
    "              'seed': 36683,\n",
    "              'nthread': 4,\n",
    "              'num_class': 3,\n",
    "              'silent': 1\n",
    "              }\n",
    "    xgb2cv = xgb.cv(params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_rounds,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    verbose_eval=50,\n",
    "                    early_stopping_rounds=early_stop_rounds)\n",
    "    return xgb2cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features finished\n",
      "Ordinal encoding finished\n",
      "['feature_elev', 'feature_cats_allow', 'feature_hardwood_floor', 'feature_dogs_allow', 'feature_doorman', 'feature_dishwash', 'feature_laundry_in_build', 'feature_no_fe', 'feature_fitness_cent', 'feature_laundry_in_unit', 'feature_pre-war', 'feature_roof_deck', 'feature_outdoor_spac', 'feature_dining_room', 'feature_high_speed_internet', 'feature_balconi', 'feature_swimming_pool', 'feature_new_construct', 'feature_terrac', 'feature_exclus', 'feature_loft', 'feature_gardenpatio', 'feature_prewar', 'feature_wheelchair_access', 'feature_common_outdoor_spac', 'feature_hardwood', 'feature_simplex', 'feature_fireplac', 'feature_high_ceil', 'feature_lowris', 'feature_garag', 'feature_reduced_fe', 'feature_laundry_room', 'feature_furnish', 'feature_multi-level', 'feature_private_outdoor_spac', 'feature_parking_spac', 'feature_publicoutdoor', 'feature_roof-deck', 'feature_live_in_sup', 'feature_renov', 'feature_pool', 'feature_on-site_laundri', 'feature_laundri', 'feature_green_build', 'feature_storag', 'feature_stainless_steel_appli', 'feature_concierg', 'feature_washer_in_unit', 'feature_dryer_in_unit', 'feature_newly_renov', 'feature_on-site_garag', 'feature_light', 'feature_patio', 'feature_washerdry', 'feature_walk_in_closet', 'feature_live-in_superintend', 'feature_gymfit', 'feature_exposed_brick', 'feature_granite_kitchen', 'feature_bike_room', 'feature_pets_on_approv', 'feature_garden', 'feature_marble_bath', 'feature_valet', 'feature_subway', 'feature_residents_loung', 'feature_eat_in_kitchen', 'feature_central_ac', 'feature_live-in_sup', 'feature_full-time_doorman', 'feature_common_parkinggarag', 'feature_wifi_access', 'feature_park', 'feature_highris', 'feature_loung', 'feature_short_term_allow', 'feature_childrens_playroom', 'feature_no_pet', 'feature_duplex', 'feature_actual_apt_photo', 'feature_view', 'feature_luxury_build', 'feature_gym', 'feature_common_roof_deck', 'feature_residents_garden', 'feature_outdoor_area', 'feature_roofdeck', 'feature_indoor_pool', \"feature_children's_playroom\", 'feature_virtual_doorman', 'feature_livework', 'feature_building-common-outdoor-spac', 'feature_washer_dry', 'feature_microwav', 'feature_sauna', 'feature_valet_park', 'feature_air_condit', 'feature_lounge_room', 'feature_sublet']\n",
      "Document-term matrix finished\n",
      "Start xgboost cross-validation\n",
      "[0]\ttrain-mlogloss:1.07868+0.000131318\ttest-mlogloss:1.0789+0.000139939\n",
      "[50]\ttrain-mlogloss:0.659322+0.000402999\ttest-mlogloss:0.669276+0.00209633\n",
      "[100]\ttrain-mlogloss:0.56903+0.000813097\ttest-mlogloss:0.586299+0.0026664\n",
      "[150]\ttrain-mlogloss:0.534686+0.00106521\ttest-mlogloss:0.558331+0.00327993\n",
      "[200]\ttrain-mlogloss:0.514557+0.00121571\ttest-mlogloss:0.544613+0.00371019\n",
      "[250]\ttrain-mlogloss:0.499798+0.00114248\ttest-mlogloss:0.536141+0.00414376\n",
      "[300]\ttrain-mlogloss:0.487697+0.00107937\ttest-mlogloss:0.530363+0.00446889\n",
      "[350]\ttrain-mlogloss:0.476945+0.00082748\ttest-mlogloss:0.525903+0.0047987\n",
      "[400]\ttrain-mlogloss:0.467564+0.000999545\ttest-mlogloss:0.5227+0.00480136\n",
      "[450]\ttrain-mlogloss:0.459062+0.00122467\ttest-mlogloss:0.520151+0.00489985\n",
      "[500]\ttrain-mlogloss:0.45107+0.00128446\ttest-mlogloss:0.518098+0.0049414\n",
      "[550]\ttrain-mlogloss:0.443395+0.00139474\ttest-mlogloss:0.516398+0.00502145\n",
      "[600]\ttrain-mlogloss:0.436306+0.00131405\ttest-mlogloss:0.514951+0.00514749\n",
      "[650]\ttrain-mlogloss:0.429667+0.00124304\ttest-mlogloss:0.513723+0.00530703\n",
      "[700]\ttrain-mlogloss:0.423162+0.00134655\ttest-mlogloss:0.512703+0.00533096\n",
      "[750]\ttrain-mlogloss:0.416893+0.001414\ttest-mlogloss:0.511865+0.00535075\n",
      "[800]\ttrain-mlogloss:0.41095+0.0014923\ttest-mlogloss:0.511081+0.00547985\n",
      "[850]\ttrain-mlogloss:0.405176+0.00152322\ttest-mlogloss:0.510343+0.0055147\n",
      "[900]\ttrain-mlogloss:0.399568+0.00158189\ttest-mlogloss:0.509788+0.00560961\n",
      "[950]\ttrain-mlogloss:0.394195+0.00162323\ttest-mlogloss:0.509234+0.00553046\n",
      "[1000]\ttrain-mlogloss:0.388965+0.00154386\ttest-mlogloss:0.508803+0.00572525\n",
      "[1050]\ttrain-mlogloss:0.383978+0.00159686\ttest-mlogloss:0.508442+0.00579366\n",
      "[1100]\ttrain-mlogloss:0.378838+0.00160398\ttest-mlogloss:0.508068+0.0059001\n",
      "[1150]\ttrain-mlogloss:0.373952+0.00162347\ttest-mlogloss:0.50777+0.00598778\n",
      "[1200]\ttrain-mlogloss:0.369208+0.00153378\ttest-mlogloss:0.507508+0.00598301\n",
      "[1250]\ttrain-mlogloss:0.364448+0.00163637\ttest-mlogloss:0.507311+0.00610167\n",
      "[1300]\ttrain-mlogloss:0.359936+0.00164937\ttest-mlogloss:0.507291+0.00607742\n",
      "[1350]\ttrain-mlogloss:0.35558+0.00160601\ttest-mlogloss:0.507125+0.00613086\n",
      "[1400]\ttrain-mlogloss:0.351203+0.00157997\ttest-mlogloss:0.506994+0.00625392\n",
      "[1450]\ttrain-mlogloss:0.346956+0.00162521\ttest-mlogloss:0.506874+0.00628593\n",
      "[1500]\ttrain-mlogloss:0.34289+0.00165065\ttest-mlogloss:0.506759+0.00641068\n",
      "[1550]\ttrain-mlogloss:0.338759+0.00170138\ttest-mlogloss:0.506719+0.00635919\n",
      "[1600]\ttrain-mlogloss:0.334783+0.00167287\ttest-mlogloss:0.506722+0.0064366\n",
      "[1650]\ttrain-mlogloss:0.330892+0.00160542\ttest-mlogloss:0.506758+0.0065473\n",
      "[1700]\ttrain-mlogloss:0.3271+0.00163704\ttest-mlogloss:0.506828+0.0066144\n",
      "[1750]\ttrain-mlogloss:0.323319+0.00159486\ttest-mlogloss:0.506855+0.00670658\n",
      "[1800]\ttrain-mlogloss:0.319653+0.00163337\ttest-mlogloss:0.506976+0.00684475\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, listing_id = xgb_data_prep()\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# cross-validation and training\n",
    "xgb_cv_hist = xgb_cv(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
