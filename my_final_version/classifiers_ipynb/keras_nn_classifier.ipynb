{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re, string, time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom functions for loading and preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_freq_vects(docs, max_feature=None, percent=None, token_pattern=u'(?u)\\b\\w\\w+\\b'):\n",
    "    vect = CountVectorizer(token_pattern=token_pattern)\n",
    "    feat_sparse = vect.fit_transform(docs.values.astype('U'))\n",
    "    freq_table = list(zip(vect.get_feature_names(), np.asarray(feat_sparse.sum(axis=0)).ravel()))\n",
    "    freq_table = pd.DataFrame(freq_table, columns=['feature', 'count']).sort_values('count', ascending=False)\n",
    "    if not max_feature:\n",
    "        if percent:\n",
    "            max_feature = int(percent * len(vect.get_feature_names()))\n",
    "        else:\n",
    "            max_feature = len(vect.get_feature_names())\n",
    "    feat_df = pd.DataFrame(feat_sparse.todense(), columns=vect.get_feature_names())\n",
    "    names = list(freq_table.feature[:max_feature])\n",
    "    return feat_df[names]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    print('Loading features files')\n",
    "    basic_feat = pd.read_json('../feat_input/basic_feat.json')\n",
    "    longtime_feat = pd.read_csv('../feat_input/longtime_feat.csv')\n",
    "    encoded_feat = pd.read_csv('../feat_input/feat_stats_encoding.csv')\n",
    "\n",
    "    # apply ordinal encoding to categorical feature\n",
    "    print('Ordinal encoding')\n",
    "    basic_feat.display_address = basic_feat.display_address.replace(r'\\r$', '', regex=True)\n",
    "    basic_feat.street_address = basic_feat.street_address.replace(r'\\r$', '', regex=True)\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "    for f in categorical:\n",
    "        if basic_feat[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(basic_feat[f].values))\n",
    "            basic_feat[f] = lbl.transform(list(basic_feat[f].values))\n",
    "\n",
    "    all_feat = basic_feat.merge(longtime_feat, on='listing_id')\n",
    "    all_feat = all_feat.merge(encoded_feat, on='listing_id')\n",
    "\n",
    "    print(\"Features document-term matrix\")\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punct = string.punctuation\n",
    "    punct = re.sub(\"'|-\", \"\", punct)\n",
    "    pattern = r\"[0-9]|[{}]\".format(punct)\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [re.sub(pattern, \"\", y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ['_'.join(['feature'] + y.split()) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ' '.join(x))\n",
    "    vect_df = most_freq_vects(all_feat['features'], max_feature=100, token_pattern=r\"[^ ]+\")\n",
    "    \n",
    "    all_feat = pd.concat([all_feat, vect_df], axis=1)\n",
    "    train = all_feat[all_feat.interest_level != -1].copy()\n",
    "    test = all_feat[all_feat.interest_level == -1].copy()\n",
    "    y_train=train[\"interest_level\"]\n",
    "\n",
    "    x_train = train.drop([\"interest_level\",\"features\"],axis=1)\n",
    "    x_test = test.drop([\"interest_level\",\"features\"],axis=1)\n",
    "\n",
    "    return x_train, y_train, x_test, x_test.columns.values, x_test.listing_id\n",
    "\n",
    "\n",
    "def _preprocess(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build layers of nn model using keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_create_model(x, reg, class_num):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=x.shape[1], activation='relu', kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(class_num, activation='softmax', kernel_regularizer=regularizers.l2(reg)))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function run model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_run_model(model, dtrain, dtest, batch_size=64, nb_epochs=20, patience=5):\n",
    "    if dtest:\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=patience, verbose=0, mode='auto')\n",
    "        model.fit(dtrain[0], dtrain[1], batch_size=batch_size, epochs=nb_epochs,\n",
    "                  callbacks=[early_stop], validation_data=dtest, verbose=0)\n",
    "        y_train_pred, y_test_pred = model.predict(dtrain[0]), model.predict(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)        \n",
    "        return model, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        model.fit(dtrain[0], dtrain[1], batch_size=batch_size, epochs=nb_epochs, verbose=2)\n",
    "        y_train_pred = model.predict(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return model, y_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with 5-fold cross-validation, output logloss for both train and validation data from cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_cv(preprocess='linear', reg=0.005, batch_size=64, nb_epochs=100, patience=5):\n",
    "    class_num, reg = 3, 0.005\n",
    "    X_train, y_train_cls, X_test, _, _ = load_data()\n",
    "    y_train = np_utils.to_categorical(y_train_cls, class_num)\n",
    "    if preprocess == 'log':\n",
    "        X_train, X_test = _preprocess_log(X_train, X_test)\n",
    "    else:\n",
    "        X_train, X_test = _preprocess(X_train, X_test)\n",
    "    \n",
    "    cv_scores, n_folds = [], 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=816)\n",
    "    for i, (train_ind, val_ind) in enumerate(skf.split(X_train, y_train_cls)):\n",
    "        print(\"Running Fold\", i + 1, \"/\", n_folds)\n",
    "        start = time.time()\n",
    "        \n",
    "        train_x, val_x = X_train.iloc[train_ind, :].as_matrix(), X_train.iloc[val_ind, :].as_matrix()\n",
    "        train_y, val_y = y_train[train_ind, :], y_train[val_ind, :]\n",
    "        clf = nn_create_model(train_x, reg, class_num)\n",
    "        clf, train_loss, val_loss = nn_run_model(clf, (train_x, train_y), (val_x, val_y), batch_size, nb_epochs, patience)\n",
    "        cv_scores.append([train_loss, val_loss])\n",
    "        \n",
    "        print(\"train_loss: {0:.6f}, val_loss: {1:.6f}\".format(train_loss, val_loss), end=\"\\t\")\n",
    "        \n",
    "        end = time.time()\n",
    "        m, s = divmod(end-start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))\n",
    "        \n",
    "    mean_train_loss = np.mean([cv_scores[i][0] for i in range(len(cv_scores))])\n",
    "    mean_val_loss = np.mean([cv_scores[i][1] for i in range(len(cv_scores))])\n",
    "    print(\"train_loss mean: {0:.6f}, val_loss mean: {1:.6f}\".format(mean_train_loss, mean_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Running Fold 1 / 5\n",
      "train_loss: 0.564195, val_loss: 0.582923\ttime elapsed: 0:02:50\n",
      "Running Fold 2 / 5\n",
      "train_loss: 0.569054, val_loss: 0.570021\ttime elapsed: 0:02:33\n",
      "Running Fold 3 / 5\n",
      "train_loss: 0.564443, val_loss: 0.576351\ttime elapsed: 0:02:53\n",
      "Running Fold 4 / 5\n",
      "train_loss: 0.566299, val_loss: 0.591557\ttime elapsed: 0:02:49\n",
      "Running Fold 5 / 5\n",
      "train_loss: 0.568864, val_loss: 0.582655\ttime elapsed: 0:01:41\n",
      "train_loss mean: 0.566571, val_loss mean: 0.580702\n"
     ]
    }
   ],
   "source": [
    "neural_network_cv(preprocess='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Running Fold 1 / 5\n",
      "train_loss: 0.540040, val_loss: 0.562592\ttime elapsed: 0:02:11\n",
      "Running Fold 2 / 5\n",
      "train_loss: 0.548724, val_loss: 0.556050\ttime elapsed: 0:01:32\n",
      "Running Fold 3 / 5\n",
      "train_loss: 0.546578, val_loss: 0.560894\ttime elapsed: 0:01:26\n",
      "Running Fold 4 / 5\n",
      "train_loss: 0.539318, val_loss: 0.560809\ttime elapsed: 0:02:29\n",
      "Running Fold 5 / 5\n",
      "train_loss: 0.541070, val_loss: 0.553596\ttime elapsed: 0:01:48\n",
      "train_loss mean: 0.543146, val_loss mean: 0.558788\n"
     ]
    }
   ],
   "source": [
    "neural_network_cv(preprocess='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `acc, loss` is computed on the training batches, `val_acc, val_loss` is calculated on the validation dataset from cv.\n",
    "- Much better result is obtained, with log(X+1) transformation of the input features. However, the neural network is still less comparable with results from boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try other input parameters: `reg=0.01, batch_size=640, nb_epochs=5000, patience=20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Running Fold 1 / 5\n",
      "train_loss: 0.518329, val_loss: 0.551731\ttime elapsed: 0:00:31\n",
      "Running Fold 2 / 5\n",
      "train_loss: 0.524863, val_loss: 0.538281\ttime elapsed: 0:00:33\n",
      "Running Fold 3 / 5\n",
      "train_loss: 0.522881, val_loss: 0.546620\ttime elapsed: 0:00:42\n",
      "Running Fold 4 / 5\n",
      "train_loss: 0.518309, val_loss: 0.549502\ttime elapsed: 0:00:57\n",
      "Running Fold 5 / 5\n",
      "train_loss: 0.522674, val_loss: 0.542417\ttime elapsed: 0:01:51\n",
      "train_loss mean: 0.521411, val_loss mean: 0.545710\n"
     ]
    }
   ],
   "source": [
    "neural_network_cv(preprocess='log', reg=0.01, batch_size=640, nb_epochs=5000, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Running Fold 1 / 5\n",
      "train_loss: 0.505150, val_loss: 0.548612\ttime elapsed: 0:00:41\n",
      "Running Fold 2 / 5\n",
      "train_loss: 0.508658, val_loss: 0.532170\ttime elapsed: 0:00:54\n",
      "Running Fold 3 / 5\n",
      "train_loss: 0.507986, val_loss: 0.542261\ttime elapsed: 0:01:01\n",
      "Running Fold 4 / 5\n",
      "train_loss: 0.506295, val_loss: 0.545356\ttime elapsed: 0:00:59\n",
      "Running Fold 5 / 5\n",
      "train_loss: 0.508341, val_loss: 0.538506\ttime elapsed: 0:00:55\n",
      "train_loss mean: 0.507286, val_loss mean: 0.541381\n"
     ]
    }
   ],
   "source": [
    "neural_network_cv(preprocess='log', reg=0.01, batch_size=6400, nb_epochs=5000, patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, neural network performs worse than xgboost by around 0.03 in logloss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
