{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re, string, time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom functions for loading and preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_freq_vects(docs, max_feature=None, percent=None, token_pattern=u'(?u)\\b\\w\\w+\\b'):\n",
    "    vect = CountVectorizer(token_pattern=token_pattern)\n",
    "    feat_sparse = vect.fit_transform(docs.values.astype('U'))\n",
    "    freq_table = list(zip(vect.get_feature_names(), np.asarray(feat_sparse.sum(axis=0)).ravel()))\n",
    "    freq_table = pd.DataFrame(freq_table, columns=['feature', 'count']).sort_values('count', ascending=False)\n",
    "    if not max_feature:\n",
    "        if percent:\n",
    "            max_feature = int(percent * len(vect.get_feature_names()))\n",
    "        else:\n",
    "            max_feature = len(vect.get_feature_names())\n",
    "    feat_df = pd.DataFrame(feat_sparse.todense(), columns=vect.get_feature_names())\n",
    "    names = list(freq_table.feature[:max_feature])\n",
    "    return feat_df[names]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    print('Loading features files')\n",
    "    basic_feat = pd.read_json('../feat_input/basic_feat.json')\n",
    "    longtime_feat = pd.read_csv('../feat_input/longtime_feat.csv')\n",
    "    encoded_feat = pd.read_csv('../feat_input/feat_stats_encoding.csv')\n",
    "\n",
    "    # apply ordinal encoding to categorical feature\n",
    "    print('Ordinal encoding')\n",
    "    basic_feat.display_address = basic_feat.display_address.replace(r'\\r$', '', regex=True)\n",
    "    basic_feat.street_address = basic_feat.street_address.replace(r'\\r$', '', regex=True)\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "    for f in categorical:\n",
    "        if basic_feat[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(basic_feat[f].values))\n",
    "            basic_feat[f] = lbl.transform(list(basic_feat[f].values))\n",
    "\n",
    "    all_feat = basic_feat.merge(longtime_feat, on='listing_id')\n",
    "    all_feat = all_feat.merge(encoded_feat, on='listing_id')\n",
    "\n",
    "    print(\"Features document-term matrix\")\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punct = string.punctuation\n",
    "    punct = re.sub(\"'|-\", \"\", punct)\n",
    "    pattern = r\"[0-9]|[{}]\".format(punct)\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [re.sub(pattern, \"\", y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ['_'.join(['feature'] + y.split()) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ' '.join(x))\n",
    "    vect_df = most_freq_vects(all_feat['features'], max_feature=100, token_pattern=r\"[^ ]+\")\n",
    "    \n",
    "    all_feat = pd.concat([all_feat, vect_df], axis=1)\n",
    "    train = all_feat[all_feat.interest_level != -1].copy()\n",
    "    test = all_feat[all_feat.interest_level == -1].copy()\n",
    "    y_train=train[\"interest_level\"]\n",
    "\n",
    "    x_train = train.drop([\"interest_level\",\"features\"],axis=1)\n",
    "    x_test = test.drop([\"interest_level\",\"features\"],axis=1)\n",
    "\n",
    "    return x_train, y_train, x_test, x_test.columns.values, x_test.listing_id\n",
    "\n",
    "\n",
    "def _preprocess(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caculate performance using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(dtrain, dtest=None, n_neighbors=5, weights='uniform'):\n",
    "    clf = KNeighborsClassifier()\n",
    "    params = {'n_neighbors': n_neighbors,\n",
    "              'weights': weights,\n",
    "              'n_jobs': -1,\n",
    "              }\n",
    "    clf.set_params(**params)\n",
    "    if dtest:\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred, y_test_pred = clf.predict_proba(dtrain[0]), clf.predict_proba(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)\n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred = clf.predict_proba(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return clf, y_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cv(preprocess='linear', n_neighbors=5, weights='uniform'):\n",
    "    X_train, y_train_cls, X_test, _, _ = load_data()\n",
    "    if preprocess == 'log':\n",
    "        X_train, X_test = _preprocess_log(X_train, X_test)\n",
    "    else:\n",
    "        X_train, X_test = _preprocess(X_train, X_test)\n",
    "\n",
    "    cv_scores, n_folds = [], 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=816)\n",
    "    for i, (train_ind, val_ind) in enumerate(skf.split(X_train, y_train_cls)):\n",
    "        print(\"Running Fold\", i + 1, \"/\", n_folds)\n",
    "        start = time.time()\n",
    "        \n",
    "        train_x, val_x = X_train.iloc[train_ind, :], X_train.iloc[val_ind, :]\n",
    "        train_y, val_y = y_train_cls.iloc[train_ind], y_train_cls.iloc[val_ind]\n",
    "        clf, train_loss, val_loss = run_model((train_x, train_y), (val_x, val_y), n_neighbors, weights)\n",
    "        cv_scores.append([train_loss, val_loss])\n",
    "        \n",
    "        print(\"train_loss: {0:.6f}, val_loss: {1:.6f}\".format(train_loss, val_loss), end=\"\\t\")\n",
    "        \n",
    "        end = time.time()\n",
    "        m, s = divmod(end-start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))\n",
    "        \n",
    "    mean_train_loss = np.mean([cv_scores[i][0] for i in range(len(cv_scores))])\n",
    "    mean_val_loss = np.mean([cv_scores[i][1] for i in range(len(cv_scores))])\n",
    "    print(\"train_loss mean: {0:.6f}, val_loss mean: {1:.6f}\".format(mean_train_loss, mean_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Running Fold 1 / 5\n",
      "train_loss: 0.630175, val_loss: 0.649817\ttime elapsed: 0:04:27\n",
      "Running Fold 2 / 5\n",
      "train_loss: 0.633348, val_loss: 0.640182\ttime elapsed: 0:03:59\n",
      "Running Fold 3 / 5\n",
      "train_loss: 0.631506, val_loss: 0.639124\ttime elapsed: 0:04:15\n",
      "Running Fold 4 / 5\n",
      "train_loss: 0.631418, val_loss: 0.641704\ttime elapsed: 0:03:50\n",
      "Running Fold 5 / 5\n",
      "train_loss: 0.631175, val_loss: 0.647124\ttime elapsed: 0:04:23\n",
      "train_loss mean: 0.631524, val_loss mean: 0.643590\n"
     ]
    }
   ],
   "source": [
    "train_cv(preprocess='log', n_neighbors=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine optimum num of neighbors using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _GridSearchCV(preprocess='linear', n_neighbors=None):\n",
    "    X_train, y_train_cls, X_test, _, _ = load_data()\n",
    "    if preprocess == 'log':\n",
    "        X_train, X_test = _preprocess_log(X_train, X_test)\n",
    "    else:\n",
    "        X_train, X_test = _preprocess(X_train, X_test)\n",
    "\n",
    "    kneigh = KNeighborsClassifier(n_jobs=-1)\n",
    "    params_grid = {'n_neighbors': n_neighbors}\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=816)\n",
    "    clf = GridSearchCV(kneigh, params_grid, scoring='neg_log_loss', n_jobs=-1, verbose=2, \n",
    "                       cv=skf.split(X_train, y_train_cls))\n",
    "    clf.fit(X_train, y_train_cls)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] n_neighbors=1 ...................................................\n",
      "[CV] n_neighbors=1 ...................................................\n",
      "[CV] n_neighbors=1 ...................................................\n",
      "[CV] n_neighbors=1 ...................................................\n",
      "[CV] .................................... n_neighbors=1, total= 2.3min\n",
      "[CV] n_neighbors=1 ...................................................\n",
      "[CV] .................................... n_neighbors=1, total= 2.3min\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] .................................... n_neighbors=1, total= 2.4min\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] .................................... n_neighbors=1, total= 2.3min\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] .................................... n_neighbors=1, total= 2.3min\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] ................................... n_neighbors=10, total= 2.8min\n",
      "[CV] n_neighbors=10 ..................................................\n",
      "[CV] ................................... n_neighbors=10, total= 2.8min\n",
      "[CV] n_neighbors=100 .................................................\n",
      "[CV] ................................... n_neighbors=10, total= 2.9min\n",
      "[CV] n_neighbors=100 .................................................\n",
      "[CV] ................................... n_neighbors=10, total= 2.8min\n",
      "[CV] n_neighbors=100 .................................................\n",
      "[CV] ................................... n_neighbors=10, total= 2.7min\n",
      "[CV] n_neighbors=100 .................................................\n",
      "[CV] .................................. n_neighbors=100, total= 3.1min\n",
      "[CV] n_neighbors=100 .................................................\n",
      "[CV] .................................. n_neighbors=100, total= 3.2min\n",
      "[CV] n_neighbors=1000 ................................................\n",
      "[CV] .................................. n_neighbors=100, total= 3.2min\n",
      "[CV] n_neighbors=1000 ................................................\n",
      "[CV] .................................. n_neighbors=100, total= 3.1min\n",
      "[CV] n_neighbors=1000 ................................................\n",
      "[CV] .................................. n_neighbors=100, total= 3.1min\n",
      "[CV] n_neighbors=1000 ................................................\n",
      "[CV] ................................. n_neighbors=1000, total= 3.6min\n",
      "[CV] n_neighbors=1000 ................................................\n",
      "[CV] ................................. n_neighbors=1000, total= 3.5min\n",
      "[CV] ................................. n_neighbors=1000, total= 3.5min\n",
      "[CV] ................................. n_neighbors=1000, total= 3.4min\n",
      "[CV] ................................. n_neighbors=1000, total= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 62.3min finished\n"
     ]
    }
   ],
   "source": [
    "clf = _GridSearchCV(preprocess='log', n_neighbors=[1,10,100,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'split3_train_score', 'split4_train_score', 'mean_train_score', 'std_train_score', 'mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_n_neighbors', 'params'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.21362486,   1.48584867,   0.6483376 ,   0.67118175])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the code is to optimize the negative logloss\n",
    "# thus take negative gives the true logloss\n",
    "-clf.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.10942375e-15,   5.00982137e-01,   6.15775669e-01,\n",
       "         6.69489245e-01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-clf.cv_results_['mean_train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_neighbors': 1},\n",
       " {'n_neighbors': 10},\n",
       " {'n_neighbors': 100},\n",
       " {'n_neighbors': 1000})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25838928,  2.37082696,  2.61748352,  2.61251588])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in seconds\n",
    "clf.cv_results_['mean_fit_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.2971593 ,  2.74915991,  3.12078474,  3.34374926])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in minutes\n",
    "clf.cv_results_['mean_score_time'] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the GridSearchCV result above, the best score is obtained around $k=100$ nearest neighbors. Thus $k\\approx 200$ is good choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
