{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking models after standardizing all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,Normalizer,StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from collections import OrderedDict, defaultdict\n",
    "import re, string, time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, stack models for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stacking(clf,train_x,train_y,test_x,clf_name,class_num=3):\n",
    "    train=np.zeros((train_x.shape[0],class_num))\n",
    "    test=np.zeros((test_x.shape[0],class_num))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],class_num))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"mnb\",\"ovr\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)\n",
    "            cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"lsvc\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.decision_function(te_x)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.decision_function(test_x)\n",
    "            cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      \"num_class\": class_num\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round, evals=watchlist,\n",
    "                                  verbose_eval=100, early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)\n",
    "                cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'feature_fraction': 0.7,\n",
    "                      'bagging_fraction': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 4,\n",
    "                      \"num_class\": class_num,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round, valid_sets=test_matrix,\n",
    "                                  verbose_eval=100, early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)\n",
    "                cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"nn\"]:\n",
    "            from keras.layers import Dense, Dropout, BatchNormalization\n",
    "            from keras.optimizers import SGD,RMSprop\n",
    "            from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "            from keras.utils import np_utils\n",
    "            from keras.regularizers import l2\n",
    "            from keras.models import Sequential\n",
    "            clf = Sequential()\n",
    "            clf.add(Dense(64, input_dim=tr_x.shape[1],activation=\"relu\", W_regularizer=l2()))\n",
    "            #clf.add(SReLU())\n",
    "            #clf.add(Dropout(0.2))\n",
    "            clf.add(Dense(64,activation=\"relu\",W_regularizer=l2()))\n",
    "            #clf.add(SReLU())\n",
    "            #clf.add(Dense(64, activation=\"relu\", W_regularizer=l2()))\n",
    "            # model.add(Dropout(0.2))\n",
    "            clf.add(Dense(class_num, activation=\"softmax\"))\n",
    "            #clf.summary()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "            reduce = ReduceLROnPlateau(min_lr=0.0002,factor=0.05)\n",
    "            clf.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "            clf.fit(tr_x, tr_y,\n",
    "                    batch_size=640,\n",
    "                    nb_epoch=1000,\n",
    "                    validation_data=[te_x, te_y],\n",
    "                    callbacks=[early_stopping,reduce],\n",
    "                    verbose=0)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)\n",
    "            cv_scores.append(log_loss(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "        with open(\"score.txt\",\"a\") as f:\n",
    "            f.write(\"%s now score is:\"%clf_name+str(cv_scores)+\"\\n\")\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    with open(\"score.txt\", \"a\") as f:\n",
    "        f.write(\"%s_score_mean:\"%clf_name+str(np.mean(cv_scores))+\"\\n\")\n",
    "    return train.reshape(-1,class_num),test.reshape(-1,class_num)\n",
    "\n",
    "def rf(x_train, y_train, x_valid):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking(randomforest, x_train, y_train, x_valid,\"rf\")\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada(x_train, y_train, x_valid):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking(adaboost, x_train, y_train, x_valid,\"ada\")\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb(x_train, y_train, x_valid):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking(gbdt, x_train, y_train, x_valid,\"gb\")\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et(x_train, y_train, x_valid):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking(extratree, x_train, y_train, x_valid,\"et\")\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def ovr(x_train, y_train, x_valid):\n",
    "    est=RandomForestClassifier(n_estimators=400, max_depth=16, n_jobs=-1, random_state=2017, max_features=\"auto\",\n",
    "                               verbose=1)\n",
    "    ovr = OneVsRestClassifier(est,n_jobs=-1)\n",
    "    ovr_train, ovr_test = stacking(ovr, x_train, y_train, x_valid,\"ovr\")\n",
    "    return ovr_train, ovr_test,\"ovr\"\n",
    "\n",
    "def xgb(x_train, y_train, x_valid):\n",
    "    xgb_train, xgb_test = stacking(xgboost, x_train, y_train, x_valid,\"xgb\")\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb(x_train, y_train, x_valid):\n",
    "    xgb_train, xgb_test = stacking(lightgbm, x_train, y_train, x_valid,\"lgb\")\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb(x_train, y_train, x_valid):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking(gnb, x_train, y_train, x_valid,\"gnb\")\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr(x_train, y_train, x_valid):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking(logisticregression, x_train, y_train, x_valid, \"lr\")\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def fm(x_train, y_train, x_valid):\n",
    "    pass\n",
    "\n",
    "\n",
    "def lsvc(x_train, y_train, x_valid):\n",
    "\n",
    "    #linearsvc=SVC(probability=True,kernel=\"linear\",random_state=2017,verbose=1)\n",
    "    #linearsvc=SVC(probability=True,kernel=\"linear\",random_state=2017,verbose=1)\n",
    "    linearsvc=LinearSVC(random_state=2017)\n",
    "    lsvc_train, lsvc_test = stacking(linearsvc, x_train, y_train, x_valid, \"lsvc\")\n",
    "    return lsvc_train, lsvc_test, \"lsvc\"\n",
    "\n",
    "def knn(x_train, y_train, x_valid):\n",
    "    #pca = PCA(n_components=10)\n",
    "    #pca.fit(x_train)\n",
    "    #x_train = pca.transform(x_train)\n",
    "    #x_valid = pca.transform(x_valid)\n",
    "\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking(kneighbors, x_train, y_train, x_valid, \"knn\")\n",
    "    return knn_train, knn_test, \"knn\"\n",
    "\n",
    "def nn(x_train, y_train, x_valid):\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    nn_train, nn_test = stacking(\"\", x_train, y_train, x_valid, \"nn\")\n",
    "    return nn_train, nn_test, \"nn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, add stacking model for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stacking_reg(clf,train_x,train_y,test_x,clf_name):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"lsvc\",\"knn\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'eval_metric': 'rmse',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  verbose_eval=100, early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            #z = clf.Dataset(test_x, label=te_y)\n",
    "            #z=test_x\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'regression_l2',\n",
    "                      'metric': 'mse',\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'feature_fraction': 0.7,\n",
    "                      'bagging_fraction': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 4,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  verbose_eval=100, early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"nn\"]:\n",
    "            from keras.layers import Dense, Dropout, BatchNormalization\n",
    "            from keras.optimizers import SGD,RMSprop\n",
    "            from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "            from keras.utils import np_utils\n",
    "            from keras.regularizers import l2\n",
    "            from keras.models import Sequential\n",
    "            clf = Sequential()\n",
    "            clf.add(Dense(64, input_dim=tr_x.shape[1], activation=\"relu\", W_regularizer=l2()))\n",
    "            # model.add(Dropout(0.2))\n",
    "            clf.add(Dense(64, activation=\"relu\", W_regularizer=l2()))\n",
    "            # model.add(Dropout(0.2))\n",
    "            clf.add(Dense(1))\n",
    "            #clf.summary()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "            reduce = ReduceLROnPlateau(min_lr=0.0002,factor=0.05)\n",
    "            clf.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "            clf.fit(tr_x, tr_y,\n",
    "                    batch_size=640,\n",
    "                    nb_epoch=5000,\n",
    "                    validation_data=[te_x, te_y],\n",
    "                    callbacks=[early_stopping, reduce],\n",
    "                    verbose=0)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "        with open(\"score.txt\",\"a\") as f:\n",
    "            f.write(\"%s now score is:\"%clf_name+str(cv_scores)+\"\\n\")\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    with open(\"score.txt\", \"a\") as f:\n",
    "        f.write(\"%s_score_mean:\"%clf_name+str(np.mean(cv_scores))+\"\\n\")\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_reg(x_train, y_train, x_valid):\n",
    "    randomforest = RandomForestRegressor(n_estimators=600, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_reg(randomforest, x_train, y_train, x_valid,\"rf\")\n",
    "    return rf_train, rf_test,\"rf_reg\"\n",
    "\n",
    "def ada_reg(x_train, y_train, x_valid):\n",
    "    adaboost = AdaBoostRegressor(n_estimators=30, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_reg(adaboost, x_train, y_train, x_valid,\"ada\")\n",
    "    return ada_train, ada_test,\"ada_reg\"\n",
    "\n",
    "def gb_reg(x_train, y_train, x_valid):\n",
    "    gbdt = GradientBoostingRegressor(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_reg(gbdt, x_train, y_train, x_valid,\"gb\")\n",
    "    return gbdt_train, gbdt_test,\"gb_reg\"\n",
    "\n",
    "def et_reg(x_train, y_train, x_valid):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=600, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_reg(extratree, x_train, y_train, x_valid,\"et\")\n",
    "    return et_train, et_test,\"et_reg\"\n",
    "\n",
    "def lr_reg(x_train, y_train, x_valid):\n",
    "    lr_reg=LinearRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking_reg(lr_reg, x_train, y_train, x_valid, \"lr\")\n",
    "    return lr_train, lr_test, \"lr_reg\"\n",
    "\n",
    "def xgb_reg(x_train, y_train, x_valid):\n",
    "    xgb_train, xgb_test = stacking_reg(xgboost, x_train, y_train, x_valid,\"xgb\")\n",
    "    return xgb_train, xgb_test,\"xgb_reg\"\n",
    "\n",
    "def lgb_reg(x_train, y_train, x_valid):\n",
    "    lgb_train, lgb_test = stacking_reg(lightgbm, x_train, y_train, x_valid,\"lgb\")\n",
    "    return lgb_train, lgb_test,\"lgb_reg\"\n",
    "\n",
    "def nn_reg(x_train, y_train, x_valid):\n",
    "    nn_train, nn_test = stacking_reg(\"\", x_train, y_train, x_valid, \"nn\")\n",
    "    return nn_train, nn_test, \"nn_reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom functions for loading and preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_freq_vects(docs, max_feature=None, percent=None, token_pattern=u'(?u)\\b\\w\\w+\\b'):\n",
    "    vect = CountVectorizer(token_pattern=token_pattern)\n",
    "    feat_sparse = vect.fit_transform(docs.values.astype('U'))\n",
    "    freq_table = list(zip(vect.get_feature_names(), np.asarray(feat_sparse.sum(axis=0)).ravel()))\n",
    "    freq_table = pd.DataFrame(freq_table, columns=['feature', 'count']).sort_values('count', ascending=False)\n",
    "    if not max_feature:\n",
    "        if percent:\n",
    "            max_feature = int(percent * len(vect.get_feature_names()))\n",
    "        else:\n",
    "            max_feature = len(vect.get_feature_names())\n",
    "    feat_df = pd.DataFrame(feat_sparse.todense(), columns=vect.get_feature_names())\n",
    "    names = list(freq_table.feature[:max_feature])\n",
    "    return feat_df[names]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    print('Loading features files')\n",
    "    basic_feat = pd.read_json('../feat_input/basic_feat.json')\n",
    "    longtime_feat = pd.read_csv('../feat_input/longtime_feat.csv')\n",
    "    encoded_feat = pd.read_csv('../feat_input/feat_stats_encoding.csv')\n",
    "\n",
    "    # apply ordinal encoding to categorical feature\n",
    "    print('Ordinal encoding')\n",
    "    basic_feat.display_address = basic_feat.display_address.replace(r'\\r$', '', regex=True)\n",
    "    basic_feat.street_address = basic_feat.street_address.replace(r'\\r$', '', regex=True)\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "    for f in categorical:\n",
    "        if basic_feat[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(basic_feat[f].values))\n",
    "            basic_feat[f] = lbl.transform(list(basic_feat[f].values))\n",
    "\n",
    "    all_feat = basic_feat.merge(longtime_feat, on='listing_id')\n",
    "    all_feat = all_feat.merge(encoded_feat, on='listing_id')\n",
    "\n",
    "    print(\"Features document-term matrix\")\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punct = string.punctuation\n",
    "    punct = re.sub(\"'|-\", \"\", punct)\n",
    "    pattern = r\"[0-9]|[{}]\".format(punct)\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [re.sub(pattern, \"\", y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ['_'.join(['feature'] + y.split()) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ' '.join(x))\n",
    "    vect_df = most_freq_vects(all_feat['features'], max_feature=100, token_pattern=r\"[^ ]+\")\n",
    "\n",
    "    all_feat = pd.concat([all_feat, vect_df], axis=1)\n",
    "    train = all_feat[all_feat['interest_level'] != -1].copy()\n",
    "    test = all_feat[all_feat['interest_level'] == -1].copy()\n",
    "    y_train = train[\"interest_level\"]\n",
    "\n",
    "    x_train = train.drop([\"interest_level\", \"features\"], axis=1)\n",
    "    x_test = test.drop([\"interest_level\", \"features\"], axis=1)\n",
    "\n",
    "    return x_train, y_train, x_test, x_test.columns.values, x_test.listing_id\n",
    "\n",
    "def _preprocess(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for feature selection from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "def select_feature(clf,x_train,x_valid):\n",
    "    clf.fit(x_train, y_train)\n",
    "    model = SelectFromModel(clf, prefit=True, threshold=\"mean\")\n",
    "\n",
    "    print(x_train.shape)\n",
    "    x_train = model.transform(x_train)\n",
    "    x_valid = model.transform(x_valid)\n",
    "    print(x_train.shape)\n",
    "\n",
    "    return x_train,x_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function perform stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Stacking model: xgb\n",
      "[0]\ttrain-mlogloss:1.0791\teval-mlogloss:1.0792\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.57008\teval-mlogloss:0.582863\n",
      "[200]\ttrain-mlogloss:0.516291\teval-mlogloss:0.541685\n",
      "[300]\ttrain-mlogloss:0.489328\teval-mlogloss:0.527523\n",
      "[400]\ttrain-mlogloss:0.46961\teval-mlogloss:0.520219\n",
      "[500]\ttrain-mlogloss:0.454066\teval-mlogloss:0.516173\n",
      "[600]\ttrain-mlogloss:0.440205\teval-mlogloss:0.512935\n",
      "[700]\ttrain-mlogloss:0.427082\teval-mlogloss:0.510214\n",
      "[800]\ttrain-mlogloss:0.415615\teval-mlogloss:0.508478\n",
      "[900]\ttrain-mlogloss:0.404596\teval-mlogloss:0.50727\n",
      "[1000]\ttrain-mlogloss:0.394071\teval-mlogloss:0.506423\n",
      "[1100]\ttrain-mlogloss:0.384257\teval-mlogloss:0.505408\n",
      "[1200]\ttrain-mlogloss:0.375146\teval-mlogloss:0.505035\n",
      "[1300]\ttrain-mlogloss:0.36607\teval-mlogloss:0.504746\n",
      "[1400]\ttrain-mlogloss:0.357319\teval-mlogloss:0.504302\n",
      "[1500]\ttrain-mlogloss:0.349314\teval-mlogloss:0.504131\n",
      "[1600]\ttrain-mlogloss:0.341521\teval-mlogloss:0.504005\n",
      "[1700]\ttrain-mlogloss:0.333948\teval-mlogloss:0.504034\n",
      "Stopping. Best iteration:\n",
      "[1626]\ttrain-mlogloss:0.339464\teval-mlogloss:0.503878\n",
      "\n",
      "xgb now score is: [0.50387752031364108]\n",
      "[0]\ttrain-mlogloss:1.07901\teval-mlogloss:1.07923\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.569389\teval-mlogloss:0.582735\n",
      "[200]\ttrain-mlogloss:0.515695\teval-mlogloss:0.542442\n",
      "[300]\ttrain-mlogloss:0.488889\teval-mlogloss:0.52862\n",
      "[400]\ttrain-mlogloss:0.469809\teval-mlogloss:0.521497\n",
      "[500]\ttrain-mlogloss:0.453599\teval-mlogloss:0.517129\n",
      "[600]\ttrain-mlogloss:0.439223\teval-mlogloss:0.514126\n",
      "[700]\ttrain-mlogloss:0.425982\teval-mlogloss:0.511768\n",
      "[800]\ttrain-mlogloss:0.413974\teval-mlogloss:0.510125\n",
      "[900]\ttrain-mlogloss:0.402759\teval-mlogloss:0.509296\n",
      "[1000]\ttrain-mlogloss:0.392091\teval-mlogloss:0.508468\n",
      "[1100]\ttrain-mlogloss:0.382223\teval-mlogloss:0.507801\n",
      "[1200]\ttrain-mlogloss:0.372636\teval-mlogloss:0.507074\n",
      "[1300]\ttrain-mlogloss:0.363402\teval-mlogloss:0.506804\n",
      "[1400]\ttrain-mlogloss:0.354625\teval-mlogloss:0.50664\n",
      "[1500]\ttrain-mlogloss:0.346489\teval-mlogloss:0.506546\n",
      "[1600]\ttrain-mlogloss:0.338706\teval-mlogloss:0.506645\n",
      "Stopping. Best iteration:\n",
      "[1545]\ttrain-mlogloss:0.342939\teval-mlogloss:0.506442\n",
      "\n",
      "xgb now score is: [0.50387752031364108, 0.50644171748285838]\n",
      "[0]\ttrain-mlogloss:1.07851\teval-mlogloss:1.07888\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.567899\teval-mlogloss:0.590727\n",
      "[200]\ttrain-mlogloss:0.513806\teval-mlogloss:0.548385\n",
      "[300]\ttrain-mlogloss:0.487148\teval-mlogloss:0.533782\n",
      "[400]\ttrain-mlogloss:0.467879\teval-mlogloss:0.526566\n",
      "[500]\ttrain-mlogloss:0.452099\teval-mlogloss:0.52172\n",
      "[600]\ttrain-mlogloss:0.43792\teval-mlogloss:0.518479\n",
      "[700]\ttrain-mlogloss:0.425191\teval-mlogloss:0.516242\n",
      "[800]\ttrain-mlogloss:0.413212\teval-mlogloss:0.514465\n",
      "[900]\ttrain-mlogloss:0.402558\teval-mlogloss:0.513113\n",
      "[1000]\ttrain-mlogloss:0.392107\teval-mlogloss:0.512111\n",
      "[1100]\ttrain-mlogloss:0.382045\teval-mlogloss:0.511082\n",
      "[1200]\ttrain-mlogloss:0.37281\teval-mlogloss:0.510372\n",
      "[1300]\ttrain-mlogloss:0.364072\teval-mlogloss:0.50993\n",
      "[1400]\ttrain-mlogloss:0.355389\teval-mlogloss:0.509577\n",
      "[1500]\ttrain-mlogloss:0.347326\teval-mlogloss:0.509274\n",
      "[1600]\ttrain-mlogloss:0.339706\teval-mlogloss:0.50935\n",
      "Stopping. Best iteration:\n",
      "[1513]\ttrain-mlogloss:0.346281\teval-mlogloss:0.509261\n",
      "\n",
      "xgb now score is: [0.50387752031364108, 0.50644171748285838, 0.50926101054965767]\n",
      "[0]\ttrain-mlogloss:1.07865\teval-mlogloss:1.07874\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.569709\teval-mlogloss:0.582126\n",
      "[200]\ttrain-mlogloss:0.515862\teval-mlogloss:0.541513\n",
      "[300]\ttrain-mlogloss:0.489224\teval-mlogloss:0.526556\n",
      "[400]\ttrain-mlogloss:0.469916\teval-mlogloss:0.519303\n",
      "[500]\ttrain-mlogloss:0.453811\teval-mlogloss:0.515093\n",
      "[600]\ttrain-mlogloss:0.439697\teval-mlogloss:0.512017\n",
      "[700]\ttrain-mlogloss:0.426808\teval-mlogloss:0.509834\n",
      "[800]\ttrain-mlogloss:0.414963\teval-mlogloss:0.508221\n",
      "[900]\ttrain-mlogloss:0.40388\teval-mlogloss:0.507225\n",
      "[1000]\ttrain-mlogloss:0.393208\teval-mlogloss:0.506491\n",
      "[1100]\ttrain-mlogloss:0.383347\teval-mlogloss:0.505985\n",
      "[1200]\ttrain-mlogloss:0.373886\teval-mlogloss:0.505541\n",
      "[1300]\ttrain-mlogloss:0.364934\teval-mlogloss:0.505158\n",
      "[1400]\ttrain-mlogloss:0.356453\teval-mlogloss:0.504921\n",
      "[1500]\ttrain-mlogloss:0.348363\teval-mlogloss:0.504903\n",
      "Stopping. Best iteration:\n",
      "[1450]\ttrain-mlogloss:0.352276\teval-mlogloss:0.504827\n",
      "\n",
      "xgb now score is: [0.50387752031364108, 0.50644171748285838, 0.50926101054965767, 0.50482712069655999]\n",
      "[0]\ttrain-mlogloss:1.07851\teval-mlogloss:1.07902\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.568542\teval-mlogloss:0.588893\n",
      "[200]\ttrain-mlogloss:0.514728\teval-mlogloss:0.547141\n",
      "[300]\ttrain-mlogloss:0.48856\teval-mlogloss:0.532473\n",
      "[400]\ttrain-mlogloss:0.469324\teval-mlogloss:0.523883\n",
      "[500]\ttrain-mlogloss:0.453045\teval-mlogloss:0.518422\n",
      "[600]\ttrain-mlogloss:0.43892\teval-mlogloss:0.514758\n",
      "[700]\ttrain-mlogloss:0.42602\teval-mlogloss:0.512212\n",
      "[800]\ttrain-mlogloss:0.414431\teval-mlogloss:0.510131\n",
      "[900]\ttrain-mlogloss:0.403303\teval-mlogloss:0.508434\n",
      "[1000]\ttrain-mlogloss:0.392691\teval-mlogloss:0.507267\n",
      "[1100]\ttrain-mlogloss:0.382831\teval-mlogloss:0.506464\n",
      "[1200]\ttrain-mlogloss:0.373546\teval-mlogloss:0.505772\n",
      "[1300]\ttrain-mlogloss:0.364593\teval-mlogloss:0.505294\n",
      "[1400]\ttrain-mlogloss:0.355923\teval-mlogloss:0.504855\n",
      "[1500]\ttrain-mlogloss:0.347792\teval-mlogloss:0.50456\n",
      "[1600]\ttrain-mlogloss:0.339956\teval-mlogloss:0.50406\n",
      "[1700]\ttrain-mlogloss:0.332421\teval-mlogloss:0.503961\n",
      "Stopping. Best iteration:\n",
      "[1690]\ttrain-mlogloss:0.333183\teval-mlogloss:0.503852\n",
      "\n",
      "xgb now score is: [0.50387752031364108, 0.50644171748285838, 0.50926101054965767, 0.50482712069655999, 0.50385166021967731]\n",
      "xgb_score_list: [0.50387752031364108, 0.50644171748285838, 0.50926101054965767, 0.50482712069655999, 0.50385166021967731]\n",
      "xgb_score_mean: 0.505651805852\n",
      "Stacking model: nn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:92: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, input_dim=192, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:95: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72384/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.55528223652869657]\n",
      "73792/74659 [============================>.] - ETA: 0s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.55528223652869657, 0.56476227084721509]\n",
      "71776/74659 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.55528223652869657, 0.56476227084721509, 0.57247125971210078]\n",
      "72768/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.55528223652869657, 0.56476227084721509, 0.57247125971210078, 0.55504485355804023]\n",
      "72672/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.55528223652869657, 0.56476227084721509, 0.57247125971210078, 0.55504485355804023, 0.56215258669868262]\n",
      "nn_score_list: [0.55528223652869657, 0.56476227084721509, 0.57247125971210078, 0.55504485355804023, 0.56215258669868262]\n",
      "nn_score_mean: 0.561942641469\n",
      "Stacking model: lgb\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.574384\n",
      "[200]\tvalid_0's multi_logloss: 0.531415\n",
      "[300]\tvalid_0's multi_logloss: 0.518784\n",
      "[400]\tvalid_0's multi_logloss: 0.512339\n",
      "[500]\tvalid_0's multi_logloss: 0.508763\n",
      "[600]\tvalid_0's multi_logloss: 0.506915\n",
      "[700]\tvalid_0's multi_logloss: 0.506045\n",
      "[800]\tvalid_0's multi_logloss: 0.505313\n",
      "[900]\tvalid_0's multi_logloss: 0.504877\n",
      "[1000]\tvalid_0's multi_logloss: 0.504617\n",
      "[1100]\tvalid_0's multi_logloss: 0.50441\n",
      "Early stopping, best iteration is:\n",
      "[1080]\tvalid_0's multi_logloss: 0.504375\n",
      "lgb now score is: [0.50437531079273012]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.573847\n",
      "[200]\tvalid_0's multi_logloss: 0.531158\n",
      "[300]\tvalid_0's multi_logloss: 0.518733\n",
      "[400]\tvalid_0's multi_logloss: 0.513385\n",
      "[500]\tvalid_0's multi_logloss: 0.510491\n",
      "[600]\tvalid_0's multi_logloss: 0.508819\n",
      "[700]\tvalid_0's multi_logloss: 0.507776\n",
      "[800]\tvalid_0's multi_logloss: 0.507156\n",
      "[900]\tvalid_0's multi_logloss: 0.506693\n",
      "[1000]\tvalid_0's multi_logloss: 0.50649\n",
      "[1100]\tvalid_0's multi_logloss: 0.506206\n",
      "[1200]\tvalid_0's multi_logloss: 0.506072\n",
      "[1300]\tvalid_0's multi_logloss: 0.50633\n",
      "Early stopping, best iteration is:\n",
      "[1212]\tvalid_0's multi_logloss: 0.506048\n",
      "lgb now score is: [0.50437531079273012, 0.50604755728879736]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.580602\n",
      "[200]\tvalid_0's multi_logloss: 0.536825\n",
      "[300]\tvalid_0's multi_logloss: 0.523296\n",
      "[400]\tvalid_0's multi_logloss: 0.517322\n",
      "[500]\tvalid_0's multi_logloss: 0.51445\n",
      "[600]\tvalid_0's multi_logloss: 0.512634\n",
      "[700]\tvalid_0's multi_logloss: 0.511627\n",
      "[800]\tvalid_0's multi_logloss: 0.510867\n",
      "[900]\tvalid_0's multi_logloss: 0.510602\n",
      "[1000]\tvalid_0's multi_logloss: 0.510296\n",
      "[1100]\tvalid_0's multi_logloss: 0.510079\n",
      "Early stopping, best iteration is:\n",
      "[1096]\tvalid_0's multi_logloss: 0.510061\n",
      "lgb now score is: [0.50437531079273012, 0.50604755728879736, 0.51006064461890632]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.573127\n",
      "[200]\tvalid_0's multi_logloss: 0.53041\n",
      "[300]\tvalid_0's multi_logloss: 0.51671\n",
      "[400]\tvalid_0's multi_logloss: 0.510741\n",
      "[500]\tvalid_0's multi_logloss: 0.507888\n",
      "[600]\tvalid_0's multi_logloss: 0.506294\n",
      "[700]\tvalid_0's multi_logloss: 0.505288\n",
      "[800]\tvalid_0's multi_logloss: 0.504697\n",
      "[900]\tvalid_0's multi_logloss: 0.504157\n",
      "[1000]\tvalid_0's multi_logloss: 0.503773\n",
      "[1100]\tvalid_0's multi_logloss: 0.503694\n",
      "[1200]\tvalid_0's multi_logloss: 0.503794\n",
      "Early stopping, best iteration is:\n",
      "[1107]\tvalid_0's multi_logloss: 0.503623\n",
      "lgb now score is: [0.50437531079273012, 0.50604755728879736, 0.51006064461890632, 0.50362311830530115]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.579085\n",
      "[200]\tvalid_0's multi_logloss: 0.535362\n",
      "[300]\tvalid_0's multi_logloss: 0.521375\n",
      "[400]\tvalid_0's multi_logloss: 0.514576\n",
      "[500]\tvalid_0's multi_logloss: 0.510456\n",
      "[600]\tvalid_0's multi_logloss: 0.508095\n",
      "[700]\tvalid_0's multi_logloss: 0.506606\n",
      "[800]\tvalid_0's multi_logloss: 0.505461\n",
      "[900]\tvalid_0's multi_logloss: 0.504853\n",
      "[1000]\tvalid_0's multi_logloss: 0.504252\n",
      "[1100]\tvalid_0's multi_logloss: 0.50375\n",
      "[1200]\tvalid_0's multi_logloss: 0.503474\n",
      "[1300]\tvalid_0's multi_logloss: 0.50337\n",
      "[1400]\tvalid_0's multi_logloss: 0.503196\n",
      "[1500]\tvalid_0's multi_logloss: 0.503164\n",
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's multi_logloss: 0.503085\n",
      "lgb now score is: [0.50437531079273012, 0.50604755728879736, 0.51006064461890632, 0.50362311830530115, 0.50308546160914824]\n",
      "lgb_score_list: [0.50437531079273012, 0.50604755728879736, 0.51006064461890632, 0.50362311830530115, 0.50308546160914824]\n",
      "lgb_score_mean: 0.505438418523\n",
      "Stacking model: knn\n",
      "knn now score is: [0.65186285498164642]\n",
      "knn now score is: [0.65186285498164642, 0.65352390377017711]\n",
      "knn now score is: [0.65186285498164642, 0.65352390377017711, 0.6578205370141802]\n",
      "knn now score is: [0.65186285498164642, 0.65352390377017711, 0.6578205370141802, 0.65078618739176719]\n",
      "knn now score is: [0.65186285498164642, 0.65352390377017711, 0.6578205370141802, 0.65078618739176719, 0.65495210214799515]\n",
      "knn_score_list: [0.65186285498164642, 0.65352390377017711, 0.6578205370141802, 0.65078618739176719, 0.65495210214799515]\n",
      "knn_score_mean: 0.653789117061\n",
      "Stacking model: gb\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       28586.5944         169.9987            3.45m\n",
      "         2       27945.2202         159.1642            3.42m\n",
      "         3       27310.8176         145.5083            3.38m\n",
      "         4       26746.9869         135.9889            3.36m\n",
      "         5       26235.7865         126.9951            3.33m\n",
      "         6       25782.3675         120.3269            3.30m\n",
      "         7       25296.0275         110.3202            3.26m\n",
      "         8       24815.2387         102.6210            3.23m\n",
      "         9       24455.2624          96.2448            3.19m\n",
      "        10       24108.7250          91.1257            3.16m\n",
      "        20       21255.1599          47.8091            2.79m\n",
      "        30       19665.0835          27.8281            2.44m\n",
      "        40       18587.7614          17.5674            2.08m\n",
      "        50       17866.1663          11.0701            1.73m\n",
      "        60       17391.9547           8.4677            1.38m\n",
      "        70       17016.9075           4.7761            1.03m\n",
      "        80       16664.3262           4.7768           41.11s\n",
      "        90       16361.5975           2.7771           20.51s\n",
      "       100       16066.2257           1.9129            0.00s\n",
      "gb now score is: [0.5451621079453568]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       28601.0257         171.7408            3.51m\n",
      "         2       27906.8639         156.4929            3.46m\n",
      "         3       27356.2262         148.0843            3.42m\n",
      "         4       26779.3908         136.7817            3.38m\n",
      "         5       26276.6243         126.4421            3.35m\n",
      "         6       25759.0993         117.1992            3.32m\n",
      "         7       25304.8786         112.3668            3.28m\n",
      "         8       24924.2806         104.8387            3.25m\n",
      "         9       24458.3743          95.3458            3.21m\n",
      "        10       24099.3884          88.4167            3.18m\n",
      "        20       21320.7643          50.1876            2.81m\n",
      "        30       19690.1759          27.9452            2.44m\n",
      "        40       18537.9219          17.9261            2.09m\n",
      "        50       17897.0859          10.3268            1.74m\n",
      "        60       17388.1972           9.2806            1.38m\n",
      "        70       16895.2220           4.3846            1.03m\n",
      "        80       16650.9656           4.4334           41.26s\n",
      "        90       16263.2066           2.9531           20.53s\n",
      "       100       16077.5145           1.7342            0.00s\n",
      "gb now score is: [0.5451621079453568, 0.54639133487722669]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       28531.1698         171.6629            3.47m\n",
      "         2       27892.6836         162.4109            3.45m\n",
      "         3       27255.7254         146.4670            3.41m\n",
      "         4       26740.4820         140.2470            3.37m\n",
      "         5       26160.6388         125.7104            3.34m\n",
      "         6       25715.7261         119.7116            3.31m\n",
      "         7       25235.9721         111.3369            3.28m\n",
      "         8       24763.3814         102.7723            3.25m\n",
      "         9       24356.0951          95.2841            3.21m\n",
      "        10       24016.1698          90.9241            3.18m\n",
      "        20       21271.1383          48.4726            2.80m\n",
      "        30       19602.1922          28.2916            2.44m\n",
      "        40       18590.0653          19.0192            2.08m\n",
      "        50       17823.9845          11.8331            1.73m\n",
      "        60       17395.4070           7.4484            1.38m\n",
      "        70       16920.4627           5.4820            1.03m\n",
      "        80       16601.8490           5.4753           41.18s\n",
      "        90       16203.5225           1.9497           20.54s\n",
      "       100       15995.1916           2.7263            0.00s\n",
      "gb now score is: [0.5451621079453568, 0.54639133487722669, 0.55295796299880884]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       28565.3121         172.6484            3.46m\n",
      "         2       27966.2407         157.2038            3.46m\n",
      "         3       27352.5275         148.8776            3.42m\n",
      "         4       26765.7573         137.4643            3.38m\n",
      "         5       26209.0254         126.0946            3.34m\n",
      "         6       25754.0013         118.5036            3.32m\n",
      "         7       25319.3088         112.3077            3.28m\n",
      "         8       24861.6451         101.8892            3.24m\n",
      "         9       24487.3768          98.6924            3.21m\n",
      "        10       24060.3273          88.5945            3.17m\n",
      "        20       21343.3853          49.0484            2.79m\n",
      "        30       19681.0141          26.2253            2.44m\n",
      "        40       18635.0432          18.4334            2.08m\n",
      "        50       17921.0702          11.6880            1.72m\n",
      "        60       17399.1790           7.8014            1.38m\n",
      "        70       16965.0919           4.0846            1.03m\n",
      "        80       16598.2453           3.1844           41.08s\n",
      "        90       16320.6920           3.1617           20.50s\n",
      "       100       16116.0018           1.5820            0.00s\n",
      "gb now score is: [0.5451621079453568, 0.54639133487722669, 0.55295796299880884, 0.5442234427806506]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       28576.4410         172.0725            3.45m\n",
      "         2       27940.9203         159.9195            3.45m\n",
      "         3       27292.3795         149.0040            3.42m\n",
      "         4       26728.3647         136.4432            3.39m\n",
      "         5       26187.7464         125.9670            3.35m\n",
      "         6       25694.8585         117.8778            3.31m\n",
      "         7       25322.2977         113.0827            3.27m\n",
      "         8       24789.6695         102.3785            3.24m\n",
      "         9       24316.8142          96.2397            3.20m\n",
      "        10       24054.9868          90.2909            3.16m\n",
      "        20       21263.3224          49.3291            2.79m\n",
      "        30       19604.0621          27.2496            2.43m\n",
      "        40       18565.6188          18.4247            2.08m\n",
      "        50       17884.4471          12.7982            1.73m\n",
      "        60       17324.0755           6.5983            1.37m\n",
      "        70       16900.3366           4.0335            1.03m\n",
      "        80       16541.8200           3.8001           41.00s\n",
      "        90       16332.4261           2.4976           20.45s\n",
      "       100       16028.9535           1.8142            0.00s\n",
      "gb now score is: [0.5451621079453568, 0.54639133487722669, 0.55295796299880884, 0.5442234427806506, 0.55121310122275058]\n",
      "gb_score_list: [0.5451621079453568, 0.54639133487722669, 0.55295796299880884, 0.5442234427806506, 0.55121310122275058]\n",
      "gb_score_mean: 0.547989589965\n",
      "Stacking model: rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   42.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.54770750711211125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   44.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.54770750711211125, 0.5520213835254556]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   42.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.54770750711211125, 0.5520213835254556, 0.55770510360942338]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   27.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   41.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.54770750711211125, 0.5520213835254556, 0.55770510360942338, 0.54814620289581673]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   27.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   41.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.54770750711211125, 0.5520213835254556, 0.55770510360942338, 0.54814620289581673, 0.54932857275534286]\n",
      "rf_score_list: [0.54770750711211125, 0.5520213835254556, 0.55770510360942338, 0.54814620289581673, 0.54932857275534286]\n",
      "rf_score_mean: 0.55098175398\n",
      "Stacking model: et\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   34.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.57408780817254501]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   34.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    8.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.57408780817254501, 0.57874948714619023]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   35.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.57408780817254501, 0.57874948714619023, 0.58318979029242501]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   35.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.57408780817254501, 0.57874948714619023, 0.58318979029242501, 0.57942974296646077]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:   35.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.57408780817254501, 0.57874948714619023, 0.58318979029242501, 0.57942974296646077, 0.57690817695760643]\n",
      "et_score_list: [0.57408780817254501, 0.57874948714619023, 0.58318979029242501, 0.57942974296646077, 0.57690817695760643]\n",
      "et_score_mean: 0.578473001107\n",
      "Stacking model: lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr now score is: [0.58318936436249758]\n",
      "lr now score is: [0.58318936436249758, 0.58978812208627596]\n",
      "lr now score is: [0.58318936436249758, 0.58978812208627596, 0.60055621503372958]\n",
      "lr now score is: [0.58318936436249758, 0.58978812208627596, 0.60055621503372958, 0.58285861216844537]\n",
      "lr now score is: [0.58318936436249758, 0.58978812208627596, 0.60055621503372958, 0.58285861216844537, 0.58801050470240979]\n",
      "lr_score_list: [0.58318936436249758, 0.58978812208627596, 0.60055621503372958, 0.58285861216844537, 0.58801050470240979]\n",
      "lr_score_mean: 0.588880563671\n",
      "Stacking model: ada_reg\n",
      "ada now score is: [0.29177049713541114]\n",
      "ada now score is: [0.29177049713541114, 0.28928966627874614]\n",
      "ada now score is: [0.29177049713541114, 0.28928966627874614, 0.2947229185980848]\n",
      "ada now score is: [0.29177049713541114, 0.28928966627874614, 0.2947229185980848, 0.28541868843252599]\n",
      "ada now score is: [0.29177049713541114, 0.28928966627874614, 0.2947229185980848, 0.28541868843252599, 0.29591269700516049]\n",
      "ada_score_list: [0.29177049713541114, 0.28928966627874614, 0.2947229185980848, 0.28541868843252599, 0.29591269700516049]\n",
      "ada_score_mean: 0.29142289349\n",
      "Stacking model: rf_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  4.2min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.22257941246539983]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  4.2min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.22257941246539983, 0.22673131908933916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.22257941246539983, 0.22673131908933916, 0.22460845004853705]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.22257941246539983, 0.22673131908933916, 0.22460845004853705, 0.22198636745993897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf now score is: [0.22257941246539983, 0.22673131908933916, 0.22460845004853705, 0.22198636745993897, 0.22162406564617623]\n",
      "rf_score_list: [0.22257941246539983, 0.22673131908933916, 0.22460845004853705, 0.22198636745993897, 0.22162406564617623]\n",
      "rf_score_mean: 0.223505922942\n",
      "Stacking model: gb_reg\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3822           0.0092            1.17m\n",
      "         2           0.3738           0.0090            1.16m\n",
      "         3           0.3656           0.0083            1.15m\n",
      "         4           0.3560           0.0078            1.14m\n",
      "         5           0.3494           0.0065            1.13m\n",
      "         6           0.3427           0.0065            1.12m\n",
      "         7           0.3346           0.0062            1.10m\n",
      "         8           0.3290           0.0059            1.09m\n",
      "         9           0.3233           0.0055            1.08m\n",
      "        10           0.3182           0.0050            1.07m\n",
      "        20           0.2805           0.0026           56.78s\n",
      "        30           0.2565           0.0016           49.20s\n",
      "        40           0.2452           0.0008           41.98s\n",
      "        50           0.2327           0.0006           34.75s\n",
      "        60           0.2250           0.0004           27.67s\n",
      "        70           0.2197           0.0004           20.60s\n",
      "        80           0.2161           0.0002           13.67s\n",
      "        90           0.2118           0.0001            6.80s\n",
      "       100           0.2079           0.0001            0.00s\n",
      "gb now score is: [0.22312231258462861]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3817           0.0095            1.15m\n",
      "         2           0.3732           0.0088            1.14m\n",
      "         3           0.3631           0.0081            1.13m\n",
      "         4           0.3553           0.0078            1.13m\n",
      "         5           0.3494           0.0070            1.12m\n",
      "         6           0.3436           0.0065            1.10m\n",
      "         7           0.3352           0.0064            1.10m\n",
      "         8           0.3295           0.0059            1.08m\n",
      "         9           0.3243           0.0053            1.07m\n",
      "        10           0.3176           0.0052            1.06m\n",
      "        20           0.2790           0.0026           55.97s\n",
      "        30           0.2593           0.0015           48.86s\n",
      "        40           0.2432           0.0010           41.69s\n",
      "        50           0.2339           0.0006           34.59s\n",
      "        60           0.2270           0.0006           27.59s\n",
      "        70           0.2181           0.0004           20.60s\n",
      "        80           0.2142           0.0002           13.65s\n",
      "        90           0.2094           0.0002            6.80s\n",
      "       100           0.2086           0.0003            0.00s\n",
      "gb now score is: [0.22312231258462861, 0.22749157468569006]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3801           0.0093            1.17m\n",
      "         2           0.3737           0.0087            1.16m\n",
      "         3           0.3652           0.0078            1.15m\n",
      "         4           0.3545           0.0077            1.14m\n",
      "         5           0.3474           0.0071            1.13m\n",
      "         6           0.3401           0.0067            1.12m\n",
      "         7           0.3359           0.0059            1.11m\n",
      "         8           0.3298           0.0057            1.09m\n",
      "         9           0.3240           0.0056            1.08m\n",
      "        10           0.3166           0.0050            1.07m\n",
      "        20           0.2806           0.0027           56.46s\n",
      "        30           0.2579           0.0014           49.05s\n",
      "        40           0.2435           0.0008           41.85s\n",
      "        50           0.2325           0.0006           34.67s\n",
      "        60           0.2244           0.0005           27.62s\n",
      "        70           0.2186           0.0002           20.56s\n",
      "        80           0.2160           0.0003           13.65s\n",
      "        90           0.2114           0.0003            6.79s\n",
      "       100           0.2067           0.0001            0.00s\n",
      "gb now score is: [0.22312231258462861, 0.22749157468569006, 0.22583379487773803]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3811           0.0102            1.15m\n",
      "         2           0.3774           0.0086            1.16m\n",
      "         3           0.3647           0.0082            1.15m\n",
      "         4           0.3552           0.0080            1.13m\n",
      "         5           0.3514           0.0071            1.12m\n",
      "         6           0.3433           0.0067            1.11m\n",
      "         7           0.3394           0.0060            1.10m\n",
      "         8           0.3318           0.0056            1.09m\n",
      "         9           0.3254           0.0056            1.07m\n",
      "        10           0.3186           0.0051            1.06m\n",
      "        20           0.2802           0.0027           56.15s\n",
      "        30           0.2588           0.0015           48.88s\n",
      "        40           0.2431           0.0009           41.58s\n",
      "        50           0.2332           0.0006           34.61s\n",
      "        60           0.2272           0.0004           27.54s\n",
      "        70           0.2184           0.0002           20.53s\n",
      "        80           0.2170           0.0002           13.63s\n",
      "        90           0.2130           0.0003            6.77s\n",
      "       100           0.2088           0.0001            0.00s\n",
      "gb now score is: [0.22312231258462861, 0.22749157468569006, 0.22583379487773803, 0.22079597751426655]\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3834           0.0092            1.15m\n",
      "         2           0.3723           0.0092            1.15m\n",
      "         3           0.3625           0.0081            1.13m\n",
      "         4           0.3577           0.0074            1.12m\n",
      "         5           0.3509           0.0068            1.11m\n",
      "         6           0.3419           0.0066            1.10m\n",
      "         7           0.3369           0.0059            1.09m\n",
      "         8           0.3333           0.0055            1.08m\n",
      "         9           0.3268           0.0051            1.07m\n",
      "        10           0.3185           0.0050            1.06m\n",
      "        20           0.2783           0.0026           55.86s\n",
      "        30           0.2570           0.0014           48.74s\n",
      "        40           0.2421           0.0009           41.54s\n",
      "        50           0.2320           0.0008           34.43s\n",
      "        60           0.2253           0.0004           27.43s\n",
      "        70           0.2187           0.0003           20.45s\n",
      "        80           0.2147           0.0003           13.56s\n",
      "        90           0.2126           0.0001            6.76s\n",
      "       100           0.2077           0.0002            0.00s\n",
      "gb now score is: [0.22312231258462861, 0.22749157468569006, 0.22583379487773803, 0.22079597751426655, 0.22405535875558802]\n",
      "gb_score_list: [0.22312231258462861, 0.22749157468569006, 0.22583379487773803, 0.22079597751426655, 0.22405535875558802]\n",
      "gb_score_mean: 0.224259803684\n",
      "Stacking model: et_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   44.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.22351990000742863]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.22351990000742863, 0.22656312837080539]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   44.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.22351990000742863, 0.22656312837080539, 0.22372750968940733]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.22351990000742863, 0.22656312837080539, 0.22372750968940733, 0.22190639691658776]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 600 out of 600 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et now score is: [0.22351990000742863, 0.22656312837080539, 0.22372750968940733, 0.22190639691658776, 0.21968379012482411]\n",
      "et_score_list: [0.22351990000742863, 0.22656312837080539, 0.22372750968940733, 0.22190639691658776, 0.21968379012482411]\n",
      "et_score_mean: 0.223080145022\n",
      "Stacking model: xgb_reg\n",
      "[0]\ttrain-rmse:0.63094\teval-rmse:0.632719\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.471085\teval-rmse:0.480355\n",
      "[200]\ttrain-rmse:0.450726\teval-rmse:0.466281\n",
      "[300]\ttrain-rmse:0.439264\teval-rmse:0.46145\n",
      "[400]\ttrain-rmse:0.430489\teval-rmse:0.457981\n",
      "[500]\ttrain-rmse:0.423524\teval-rmse:0.456037\n",
      "[600]\ttrain-rmse:0.417267\teval-rmse:0.454485\n",
      "[700]\ttrain-rmse:0.411462\teval-rmse:0.452929\n",
      "[800]\ttrain-rmse:0.406383\teval-rmse:0.452088\n",
      "[900]\ttrain-rmse:0.401447\teval-rmse:0.451326\n",
      "[1000]\ttrain-rmse:0.397233\teval-rmse:0.450841\n",
      "[1100]\ttrain-rmse:0.392593\teval-rmse:0.450386\n",
      "[1200]\ttrain-rmse:0.388581\teval-rmse:0.449843\n",
      "[1300]\ttrain-rmse:0.385149\teval-rmse:0.449457\n",
      "[1400]\ttrain-rmse:0.38139\teval-rmse:0.449175\n",
      "[1500]\ttrain-rmse:0.377832\teval-rmse:0.448721\n",
      "[1600]\ttrain-rmse:0.374484\teval-rmse:0.448354\n",
      "[1700]\ttrain-rmse:0.371597\teval-rmse:0.448129\n",
      "[1800]\ttrain-rmse:0.368649\teval-rmse:0.447986\n",
      "[1900]\ttrain-rmse:0.365834\teval-rmse:0.447967\n",
      "[2000]\ttrain-rmse:0.363167\teval-rmse:0.447817\n",
      "[2100]\ttrain-rmse:0.360671\teval-rmse:0.447649\n",
      "[2200]\ttrain-rmse:0.358651\teval-rmse:0.447384\n",
      "[2300]\ttrain-rmse:0.356505\teval-rmse:0.447341\n",
      "Stopping. Best iteration:\n",
      "[2272]\ttrain-rmse:0.35707\teval-rmse:0.447279\n",
      "\n",
      "xgb now score is: [0.20005886749000268]\n",
      "[0]\ttrain-rmse:0.630629\teval-rmse:0.633539\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.469129\teval-rmse:0.484725\n",
      "[200]\ttrain-rmse:0.448845\teval-rmse:0.471051\n",
      "[300]\ttrain-rmse:0.437181\teval-rmse:0.465723\n",
      "[400]\ttrain-rmse:0.428581\teval-rmse:0.462705\n",
      "[500]\ttrain-rmse:0.421598\teval-rmse:0.460819\n",
      "[600]\ttrain-rmse:0.415299\teval-rmse:0.459337\n",
      "[700]\ttrain-rmse:0.409401\teval-rmse:0.45814\n",
      "[800]\ttrain-rmse:0.40423\teval-rmse:0.457335\n",
      "[900]\ttrain-rmse:0.399495\teval-rmse:0.456707\n",
      "[1000]\ttrain-rmse:0.395002\teval-rmse:0.455897\n",
      "[1100]\ttrain-rmse:0.390802\teval-rmse:0.4555\n",
      "[1200]\ttrain-rmse:0.386884\teval-rmse:0.455181\n",
      "[1300]\ttrain-rmse:0.383108\teval-rmse:0.454735\n",
      "[1400]\ttrain-rmse:0.379406\teval-rmse:0.45448\n",
      "[1500]\ttrain-rmse:0.375954\teval-rmse:0.454273\n",
      "[1600]\ttrain-rmse:0.37297\teval-rmse:0.453931\n",
      "[1700]\ttrain-rmse:0.369892\teval-rmse:0.453782\n",
      "[1800]\ttrain-rmse:0.367271\teval-rmse:0.45358\n",
      "[1900]\ttrain-rmse:0.364325\teval-rmse:0.453464\n",
      "[2000]\ttrain-rmse:0.361774\teval-rmse:0.453503\n",
      "Stopping. Best iteration:\n",
      "[1935]\ttrain-rmse:0.363396\teval-rmse:0.453385\n",
      "\n",
      "xgb now score is: [0.20005886749000268, 0.20555786195705991]\n",
      "[0]\ttrain-rmse:0.630731\teval-rmse:0.633519\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.470389\teval-rmse:0.483697\n",
      "[200]\ttrain-rmse:0.449571\teval-rmse:0.469763\n",
      "[300]\ttrain-rmse:0.437446\teval-rmse:0.464517\n",
      "[400]\ttrain-rmse:0.428996\teval-rmse:0.461468\n",
      "[500]\ttrain-rmse:0.42168\teval-rmse:0.459229\n",
      "[600]\ttrain-rmse:0.415504\teval-rmse:0.457704\n",
      "[700]\ttrain-rmse:0.409526\teval-rmse:0.456438\n",
      "[800]\ttrain-rmse:0.404399\teval-rmse:0.45548\n",
      "[900]\ttrain-rmse:0.399728\teval-rmse:0.4548\n",
      "[1000]\ttrain-rmse:0.395178\teval-rmse:0.454085\n",
      "[1100]\ttrain-rmse:0.391158\teval-rmse:0.453488\n",
      "[1200]\ttrain-rmse:0.387335\teval-rmse:0.453038\n",
      "[1300]\ttrain-rmse:0.383664\teval-rmse:0.452569\n",
      "[1400]\ttrain-rmse:0.380098\teval-rmse:0.45219\n",
      "[1500]\ttrain-rmse:0.376835\teval-rmse:0.451615\n",
      "[1600]\ttrain-rmse:0.373644\teval-rmse:0.451361\n",
      "[1700]\ttrain-rmse:0.370616\teval-rmse:0.451069\n",
      "[1800]\ttrain-rmse:0.367674\teval-rmse:0.450918\n",
      "[1900]\ttrain-rmse:0.36504\teval-rmse:0.450786\n",
      "[2000]\ttrain-rmse:0.362824\teval-rmse:0.450612\n",
      "[2100]\ttrain-rmse:0.360221\teval-rmse:0.450383\n",
      "[2200]\ttrain-rmse:0.35771\teval-rmse:0.450257\n",
      "[2300]\ttrain-rmse:0.355602\teval-rmse:0.450341\n",
      "Stopping. Best iteration:\n",
      "[2203]\ttrain-rmse:0.357604\teval-rmse:0.45025\n",
      "\n",
      "xgb now score is: [0.20005886749000268, 0.20555786195705991, 0.2027250480898162]\n",
      "[0]\ttrain-rmse:0.63261\teval-rmse:0.626218\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.47137\teval-rmse:0.478001\n",
      "[200]\ttrain-rmse:0.451489\teval-rmse:0.463998\n",
      "[300]\ttrain-rmse:0.439963\teval-rmse:0.458497\n",
      "[400]\ttrain-rmse:0.431386\teval-rmse:0.455106\n",
      "[500]\ttrain-rmse:0.424\teval-rmse:0.453149\n",
      "[600]\ttrain-rmse:0.417409\teval-rmse:0.451985\n",
      "[700]\ttrain-rmse:0.411314\teval-rmse:0.450972\n",
      "[800]\ttrain-rmse:0.406186\teval-rmse:0.450224\n",
      "[900]\ttrain-rmse:0.401239\teval-rmse:0.449648\n",
      "[1000]\ttrain-rmse:0.39676\teval-rmse:0.449282\n",
      "[1100]\ttrain-rmse:0.392372\teval-rmse:0.448672\n",
      "[1200]\ttrain-rmse:0.388314\teval-rmse:0.448145\n",
      "[1300]\ttrain-rmse:0.384508\teval-rmse:0.447954\n",
      "[1400]\ttrain-rmse:0.380967\teval-rmse:0.447568\n",
      "[1500]\ttrain-rmse:0.377609\teval-rmse:0.447344\n",
      "[1600]\ttrain-rmse:0.37439\teval-rmse:0.447172\n",
      "[1700]\ttrain-rmse:0.371753\teval-rmse:0.447102\n",
      "[1800]\ttrain-rmse:0.368886\teval-rmse:0.446795\n",
      "[1900]\ttrain-rmse:0.366419\teval-rmse:0.446755\n",
      "Stopping. Best iteration:\n",
      "[1852]\ttrain-rmse:0.367582\teval-rmse:0.446645\n",
      "\n",
      "xgb now score is: [0.20005886749000268, 0.20555786195705991, 0.2027250480898162, 0.19949184584995283]\n",
      "[0]\ttrain-rmse:0.631363\teval-rmse:0.630996\n",
      "Multiple eval metrics have been passed: 'eval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.470271\teval-rmse:0.480751\n",
      "[200]\ttrain-rmse:0.450098\teval-rmse:0.466224\n",
      "[300]\ttrain-rmse:0.438759\teval-rmse:0.460453\n",
      "[400]\ttrain-rmse:0.43041\teval-rmse:0.457174\n",
      "[500]\ttrain-rmse:0.42283\teval-rmse:0.454857\n",
      "[600]\ttrain-rmse:0.416442\teval-rmse:0.453414\n",
      "[700]\ttrain-rmse:0.410613\teval-rmse:0.452382\n",
      "[800]\ttrain-rmse:0.405538\teval-rmse:0.451376\n",
      "[900]\ttrain-rmse:0.400696\teval-rmse:0.450499\n",
      "[1000]\ttrain-rmse:0.396215\teval-rmse:0.449813\n",
      "[1100]\ttrain-rmse:0.391926\teval-rmse:0.449349\n",
      "[1200]\ttrain-rmse:0.388034\teval-rmse:0.449019\n",
      "[1300]\ttrain-rmse:0.384373\teval-rmse:0.448638\n",
      "[1400]\ttrain-rmse:0.380903\teval-rmse:0.448554\n",
      "[1500]\ttrain-rmse:0.377563\teval-rmse:0.448136\n",
      "[1600]\ttrain-rmse:0.374686\teval-rmse:0.447972\n",
      "[1700]\ttrain-rmse:0.371715\teval-rmse:0.447904\n",
      "[1800]\ttrain-rmse:0.368901\teval-rmse:0.447724\n",
      "[1900]\ttrain-rmse:0.366184\teval-rmse:0.447549\n",
      "[2000]\ttrain-rmse:0.363913\teval-rmse:0.447465\n",
      "[2100]\ttrain-rmse:0.361432\teval-rmse:0.447535\n",
      "Stopping. Best iteration:\n",
      "[2005]\ttrain-rmse:0.363725\teval-rmse:0.447416\n",
      "\n",
      "xgb now score is: [0.20005886749000268, 0.20555786195705991, 0.2027250480898162, 0.19949184584995283, 0.20018119190630873]\n",
      "xgb_score_list: [0.20005886749000268, 0.20555786195705991, 0.2027250480898162, 0.19949184584995283, 0.20018119190630873]\n",
      "xgb_score_mean: 0.201602963059\n",
      "Stacking model: nn_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:85: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, input_dim=192, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:87: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn now score is: [0.23283661694037869]\n",
      "nn now score is: [0.23283661694037869, 0.24430250877933213]\n",
      "nn now score is: [0.23283661694037869, 0.24430250877933213, 0.30225855924917983]\n",
      "nn now score is: [0.23283661694037869, 0.24430250877933213, 0.30225855924917983, 0.23130604364609064]\n",
      "nn now score is: [0.23283661694037869, 0.24430250877933213, 0.30225855924917983, 0.23130604364609064, 0.23413943821984715]\n",
      "nn_score_list: [0.23283661694037869, 0.24430250877933213, 0.30225855924917983, 0.23130604364609064, 0.23413943821984715]\n",
      "nn_score_mean: 0.248968633367\n",
      "Stacking model: lgb_reg\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.225109\n",
      "[200]\tvalid_0's l2: 0.211089\n",
      "[300]\tvalid_0's l2: 0.206932\n",
      "[400]\tvalid_0's l2: 0.204858\n",
      "[500]\tvalid_0's l2: 0.203247\n",
      "[600]\tvalid_0's l2: 0.202138\n",
      "[700]\tvalid_0's l2: 0.201324\n",
      "[800]\tvalid_0's l2: 0.200705\n",
      "[900]\tvalid_0's l2: 0.200211\n",
      "[1000]\tvalid_0's l2: 0.199739\n",
      "[1100]\tvalid_0's l2: 0.199433\n",
      "[1200]\tvalid_0's l2: 0.199257\n",
      "[1300]\tvalid_0's l2: 0.199083\n",
      "[1400]\tvalid_0's l2: 0.199119\n",
      "Early stopping, best iteration is:\n",
      "[1360]\tvalid_0's l2: 0.199018\n",
      "lgb now score is: [0.1990178694958025]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.228575\n",
      "[200]\tvalid_0's l2: 0.216421\n",
      "[300]\tvalid_0's l2: 0.212681\n",
      "[400]\tvalid_0's l2: 0.210703\n",
      "[500]\tvalid_0's l2: 0.209371\n",
      "[600]\tvalid_0's l2: 0.208643\n",
      "[700]\tvalid_0's l2: 0.207916\n",
      "[800]\tvalid_0's l2: 0.207572\n",
      "[900]\tvalid_0's l2: 0.207173\n",
      "[1000]\tvalid_0's l2: 0.206909\n",
      "[1100]\tvalid_0's l2: 0.206594\n",
      "[1200]\tvalid_0's l2: 0.206463\n",
      "[1300]\tvalid_0's l2: 0.206268\n",
      "[1400]\tvalid_0's l2: 0.205992\n",
      "[1500]\tvalid_0's l2: 0.205763\n",
      "[1600]\tvalid_0's l2: 0.205595\n",
      "[1700]\tvalid_0's l2: 0.205557\n",
      "Early stopping, best iteration is:\n",
      "[1654]\tvalid_0's l2: 0.205479\n",
      "lgb now score is: [0.1990178694958025, 0.2054787431979174]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.226681\n",
      "[200]\tvalid_0's l2: 0.213763\n",
      "[300]\tvalid_0's l2: 0.209893\n",
      "[400]\tvalid_0's l2: 0.207777\n",
      "[500]\tvalid_0's l2: 0.206388\n",
      "[600]\tvalid_0's l2: 0.205159\n",
      "[700]\tvalid_0's l2: 0.204319\n",
      "[800]\tvalid_0's l2: 0.203464\n",
      "[900]\tvalid_0's l2: 0.202942\n",
      "[1000]\tvalid_0's l2: 0.202493\n",
      "[1100]\tvalid_0's l2: 0.202306\n",
      "[1200]\tvalid_0's l2: 0.20187\n",
      "[1300]\tvalid_0's l2: 0.201693\n",
      "[1400]\tvalid_0's l2: 0.2015\n",
      "[1500]\tvalid_0's l2: 0.201364\n",
      "Early stopping, best iteration is:\n",
      "[1465]\tvalid_0's l2: 0.201318\n",
      "lgb now score is: [0.1990178694958025, 0.2054787431979174, 0.20131784406919162]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.221759\n",
      "[200]\tvalid_0's l2: 0.208871\n",
      "[300]\tvalid_0's l2: 0.204798\n",
      "[400]\tvalid_0's l2: 0.202836\n",
      "[500]\tvalid_0's l2: 0.201519\n",
      "[600]\tvalid_0's l2: 0.200689\n",
      "[700]\tvalid_0's l2: 0.199941\n",
      "[800]\tvalid_0's l2: 0.199316\n",
      "[900]\tvalid_0's l2: 0.19893\n",
      "[1000]\tvalid_0's l2: 0.198731\n",
      "[1100]\tvalid_0's l2: 0.198593\n",
      "[1200]\tvalid_0's l2: 0.198487\n",
      "[1300]\tvalid_0's l2: 0.198387\n",
      "Early stopping, best iteration is:\n",
      "[1272]\tvalid_0's l2: 0.198315\n",
      "lgb now score is: [0.1990178694958025, 0.2054787431979174, 0.20131784406919162, 0.19831547290245244]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's l2: 0.224897\n",
      "[200]\tvalid_0's l2: 0.211283\n",
      "[300]\tvalid_0's l2: 0.206947\n",
      "[400]\tvalid_0's l2: 0.204568\n",
      "[500]\tvalid_0's l2: 0.202924\n",
      "[600]\tvalid_0's l2: 0.201985\n",
      "[700]\tvalid_0's l2: 0.201173\n",
      "[800]\tvalid_0's l2: 0.200365\n",
      "[900]\tvalid_0's l2: 0.199841\n",
      "[1000]\tvalid_0's l2: 0.199466\n",
      "[1100]\tvalid_0's l2: 0.199227\n",
      "[1200]\tvalid_0's l2: 0.198924\n",
      "[1300]\tvalid_0's l2: 0.198898\n",
      "[1400]\tvalid_0's l2: 0.198825\n",
      "[1500]\tvalid_0's l2: 0.198666\n",
      "[1600]\tvalid_0's l2: 0.198494\n",
      "[1700]\tvalid_0's l2: 0.198381\n",
      "[1800]\tvalid_0's l2: 0.198248\n",
      "[1900]\tvalid_0's l2: 0.198078\n",
      "[2000]\tvalid_0's l2: 0.198007\n",
      "[2100]\tvalid_0's l2: 0.197893\n",
      "[2200]\tvalid_0's l2: 0.197794\n",
      "Early stopping, best iteration is:\n",
      "[2190]\tvalid_0's l2: 0.197783\n",
      "lgb now score is: [0.1990178694958025, 0.2054787431979174, 0.20131784406919162, 0.19831547290245244, 0.19778293025267862]\n",
      "lgb_score_list: [0.1990178694958025, 0.2054787431979174, 0.20131784406919162, 0.19831547290245244, 0.19778293025267862]\n",
      "lgb_score_mean: 0.200382571984\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    np.random.seed(1)\n",
    "    x_train, y_train, x_valid, _, _ = load_data()\n",
    "\n",
    "    train_listing = x_train[\"listing_id\"].values\n",
    "    test_listing = x_valid[\"listing_id\"].values\n",
    "\n",
    "    # preprocessing with standardization\n",
    "    x_train, x_valid = _preprocess(x_train, x_valid)\n",
    "\n",
    "    # feature selection\n",
    "    '''\n",
    "    clf=GradientBoostingClassifier()\n",
    "    x_train,x_valid=select_feature(clf,x_train,x_valid)\n",
    "    train_df=pd.DataFrame(x_train)\n",
    "    test_df=pd.DataFrame(x_valid)\n",
    "    train_df[\"listing_id\"]=train[\"listing_id\"].values\n",
    "    test_df[\"listing_id\"]=valid[\"listing_id\"].values\n",
    "    train_df.to_csv(\"best_model_train_top_feature.csv\",index=None)\n",
    "    test_df.to_csv(\"best_model_test_top_feature.csv\",index=None)\n",
    "    '''\n",
    "\n",
    "    x_train, y_train, x_valid = x_train.as_matrix(), y_train.as_matrix(), x_valid.as_matrix()\n",
    "\n",
    "    folds = 5\n",
    "    seed = 1\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # models for stacking\n",
    "    clf_list = [xgb,nn,lgb,knn,gb,rf,et,lr,ada_reg,rf_reg,gb_reg,et_reg,xgb_reg,nn_reg,lgb_reg]\n",
    "    column_list = []\n",
    "    train_data_list=[]\n",
    "    test_data_list=[]\n",
    "    for clf in clf_list:\n",
    "        print('Stacking model: {}'.format(clf.__name__))\n",
    "        train_data,test_data,clf_name=clf(x_train,y_train,x_valid)\n",
    "        train_data_list.append(train_data)\n",
    "        test_data_list.append(test_data)\n",
    "        if \"reg\" in clf_name:\n",
    "            ind_num=1\n",
    "        else:\n",
    "            ind_num=3\n",
    "        for ind in range(ind_num):\n",
    "            column_list.append(\"standardscaler_%s_%s\" % (clf_name, ind))\n",
    "\n",
    "    train = np.concatenate(train_data_list, axis=1)\n",
    "    test = np.concatenate(test_data_list, axis=1)\n",
    "\n",
    "    train = pd.DataFrame(train)\n",
    "    train.columns = column_list\n",
    "    train[\"level\"] = pd.Series(y_train)\n",
    "    train[\"listing_id\"] = train_listing\n",
    "\n",
    "    test = pd.DataFrame(test)\n",
    "    test.columns = column_list\n",
    "    test[\"listing_id\"] = test_listing\n",
    "\n",
    "    train.to_csv(\"stacking_train_stdscale.csv\", index=None)\n",
    "    test.to_csv(\"stacking_test_stdscale.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf_level2():\n",
    "    np.random.seed(1)\n",
    "    x_train = pd.read_csv(\"stacking_train_stdscale.csv\")\n",
    "    y_train = x_train['level']\n",
    "    x_train = x_train.drop(['level'], axis=1)\n",
    "    x_valid = pd.read_csv(\"stacking_test_stdscale.csv\")\n",
    "    x_train, x_valid = _preprocess(x_train, x_valid)\n",
    "\n",
    "    x_train, y_train, x_valid = x_train.as_matrix(), y_train.as_matrix(), x_valid.as_matrix()\n",
    "\n",
    "    folds = 5\n",
    "    seed = 1\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # models for stacking\n",
    "    clf_list = [xgb,nn,knn,lr,lgb]\n",
    "    column_list = []\n",
    "    train_data_list=[]\n",
    "    test_data_list=[]\n",
    "    for clf in clf_list:\n",
    "        print('Train 2nd-level model: {}'.format(clf.__name__))\n",
    "        train_data,test_data,clf_name=clf(x_train,y_train,x_valid)\n",
    "        train_data_list.append(train_data)\n",
    "        test_data_list.append(test_data)\n",
    "        if \"reg\" in clf_name:\n",
    "            ind_num=1\n",
    "        else:\n",
    "            ind_num=3\n",
    "        for ind in range(ind_num):\n",
    "            column_list.append(\"standardscaler_%s_%s\" % (clf_name, ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2nd-level model: xgb\n",
      "[0]\ttrain-mlogloss:1.07418\teval-mlogloss:1.07418\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.505043\teval-mlogloss:0.516752\n",
      "[200]\ttrain-mlogloss:0.475361\teval-mlogloss:0.498135\n",
      "[300]\ttrain-mlogloss:0.462942\teval-mlogloss:0.496802\n",
      "[400]\ttrain-mlogloss:0.452458\teval-mlogloss:0.497149\n",
      "Stopping. Best iteration:\n",
      "[330]\ttrain-mlogloss:0.459927\teval-mlogloss:0.496714\n",
      "\n",
      "xgb now score is: [0.49671411228775153]\n",
      "[0]\ttrain-mlogloss:1.07418\teval-mlogloss:1.07429\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.504197\teval-mlogloss:0.519002\n",
      "[200]\ttrain-mlogloss:0.474539\teval-mlogloss:0.500735\n",
      "[300]\ttrain-mlogloss:0.462414\teval-mlogloss:0.499789\n",
      "Stopping. Best iteration:\n",
      "[280]\ttrain-mlogloss:0.464471\teval-mlogloss:0.499727\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814]\n",
      "[0]\ttrain-mlogloss:1.07385\teval-mlogloss:1.07431\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.503578\teval-mlogloss:0.522621\n",
      "[200]\ttrain-mlogloss:0.473817\teval-mlogloss:0.503675\n",
      "[300]\ttrain-mlogloss:0.46175\teval-mlogloss:0.502599\n",
      "Stopping. Best iteration:\n",
      "[299]\ttrain-mlogloss:0.461854\teval-mlogloss:0.502593\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008]\n",
      "[0]\ttrain-mlogloss:1.07397\teval-mlogloss:1.0741\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.50471\teval-mlogloss:0.518463\n",
      "[200]\ttrain-mlogloss:0.475162\teval-mlogloss:0.499052\n",
      "[300]\ttrain-mlogloss:0.463187\teval-mlogloss:0.49759\n",
      "[400]\ttrain-mlogloss:0.452215\teval-mlogloss:0.497643\n",
      "Stopping. Best iteration:\n",
      "[329]\ttrain-mlogloss:0.459936\teval-mlogloss:0.497506\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008, 0.49750612518114862]\n",
      "[0]\ttrain-mlogloss:1.074\teval-mlogloss:1.0743\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.504802\teval-mlogloss:0.518695\n",
      "[200]\ttrain-mlogloss:0.474831\teval-mlogloss:0.499178\n",
      "[300]\ttrain-mlogloss:0.462338\teval-mlogloss:0.498505\n",
      "Stopping. Best iteration:\n",
      "[263]\ttrain-mlogloss:0.466591\teval-mlogloss:0.4984\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008, 0.49750612518114862, 0.49839997481896325]\n",
      "xgb_score_list: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008, 0.49750612518114862, 0.49839997481896325]\n",
      "xgb_score_mean: 0.498988057619\n",
      "Train 2nd-level model: nn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:92: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, input_dim=32, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:95: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72896/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.49890317622216107]\n",
      "74656/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.49890317622216107, 0.5015991311414667]\n",
      "73120/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.49890317622216107, 0.5015991311414667, 0.50523793931982997]\n",
      "74080/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.49890317622216107, 0.5015991311414667, 0.50523793931982997, 0.49882312196414452]\n",
      "73920/74659 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bnn now score is: [0.49890317622216107, 0.5015991311414667, 0.50523793931982997, 0.49882312196414452, 0.49976897593151226]\n",
      "nn_score_list: [0.49890317622216107, 0.5015991311414667, 0.50523793931982997, 0.49882312196414452, 0.49976897593151226]\n",
      "nn_score_mean: 0.500866468916\n",
      "Train 2nd-level model: knn\n",
      "knn now score is: [0.52549721271571614]\n",
      "knn now score is: [0.52549721271571614, 0.52891857330449588]\n",
      "knn now score is: [0.52549721271571614, 0.52891857330449588, 0.53616795245930882]\n",
      "knn now score is: [0.52549721271571614, 0.52891857330449588, 0.53616795245930882, 0.50850059757150456]\n",
      "knn now score is: [0.52549721271571614, 0.52891857330449588, 0.53616795245930882, 0.50850059757150456, 0.52616853056040847]\n",
      "knn_score_list: [0.52549721271571614, 0.52891857330449588, 0.53616795245930882, 0.50850059757150456, 0.52616853056040847]\n",
      "knn_score_mean: 0.525050573322\n",
      "Train 2nd-level model: lr\n",
      "lr now score is: [0.50946311761200624]\n",
      "lr now score is: [0.50946311761200624, 0.51563969115456865]\n",
      "lr now score is: [0.50946311761200624, 0.51563969115456865, 0.51852308077387321]\n",
      "lr now score is: [0.50946311761200624, 0.51563969115456865, 0.51852308077387321, 0.5131052522834133]\n",
      "lr now score is: [0.50946311761200624, 0.51563969115456865, 0.51852308077387321, 0.5131052522834133, 0.50983575588290497]\n",
      "lr_score_list: [0.50946311761200624, 0.51563969115456865, 0.51852308077387321, 0.5131052522834133, 0.50983575588290497]\n",
      "lr_score_mean: 0.513313379541\n",
      "Train 2nd-level model: lgb\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.518245\n",
      "[200]\tvalid_0's multi_logloss: 0.499376\n",
      "[300]\tvalid_0's multi_logloss: 0.498458\n",
      "Early stopping, best iteration is:\n",
      "[276]\tvalid_0's multi_logloss: 0.498364\n",
      "lgb now score is: [0.49836403248111738]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.520032\n",
      "[200]\tvalid_0's multi_logloss: 0.501815\n",
      "[300]\tvalid_0's multi_logloss: 0.501001\n",
      "Early stopping, best iteration is:\n",
      "[263]\tvalid_0's multi_logloss: 0.500896\n",
      "lgb now score is: [0.49836403248111738, 0.50089620735416795]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.523742\n",
      "[200]\tvalid_0's multi_logloss: 0.504809\n",
      "[300]\tvalid_0's multi_logloss: 0.504079\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's multi_logloss: 0.503944\n",
      "lgb now score is: [0.49836403248111738, 0.50089620735416795, 0.5039443006981581]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.519206\n",
      "[200]\tvalid_0's multi_logloss: 0.499727\n",
      "[300]\tvalid_0's multi_logloss: 0.49803\n",
      "[400]\tvalid_0's multi_logloss: 0.498132\n",
      "Early stopping, best iteration is:\n",
      "[361]\tvalid_0's multi_logloss: 0.497896\n",
      "lgb now score is: [0.49836403248111738, 0.50089620735416795, 0.5039443006981581, 0.49789578202908374]\n",
      "Train until valid scores didn't improve in 100 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.519147\n",
      "[200]\tvalid_0's multi_logloss: 0.499614\n",
      "[300]\tvalid_0's multi_logloss: 0.499347\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid_0's multi_logloss: 0.499057\n",
      "lgb now score is: [0.49836403248111738, 0.50089620735416795, 0.5039443006981581, 0.49789578202908374, 0.49905713801998236]\n",
      "lgb_score_list: [0.49836403248111738, 0.50089620735416795, 0.5039443006981581, 0.49789578202908374, 0.49905713801998236]\n",
      "lgb_score_mean: 0.500031492117\n"
     ]
    }
   ],
   "source": [
    "clf_level2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clf_level2(clf=xgb):\n",
    "    np.random.seed(1)\n",
    "    x_train = pd.read_csv(\"stacking_train_stdscale.csv\")\n",
    "    y_train = x_train['level']\n",
    "    x_train = x_train.drop(['level'], axis=1)\n",
    "    x_valid = pd.read_csv(\"stacking_test_stdscale.csv\")\n",
    "    test_listing = x_valid.listing_id.values\n",
    "    x_train, x_valid = _preprocess(x_train, x_valid)\n",
    "\n",
    "    x_train, y_train, x_valid = x_train.as_matrix(), y_train.as_matrix(), x_valid.as_matrix()\n",
    "\n",
    "    folds = 5\n",
    "    seed = 1\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # models for stacking\n",
    "    test_data_list=[]\n",
    "    print('Train 2nd-level model: {}'.format(clf.__name__))\n",
    "    _, test_data, _=clf(x_train,y_train,x_valid)\n",
    "    test_data_list.append(test_data)\n",
    "\n",
    "    test = np.concatenate(test_data_list, axis=1)\n",
    "    test = pd.DataFrame(test)\n",
    "    test.columns = ['low', 'medium', 'high']\n",
    "    test[\"listing_id\"] = test_listing\n",
    "\n",
    "    test.to_csv(\"submission_stacking_xgb.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2nd-level model: xgb\n",
      "[0]\ttrain-mlogloss:1.07418\teval-mlogloss:1.07418\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.505043\teval-mlogloss:0.516752\n",
      "[200]\ttrain-mlogloss:0.475361\teval-mlogloss:0.498135\n",
      "[300]\ttrain-mlogloss:0.462942\teval-mlogloss:0.496802\n",
      "[400]\ttrain-mlogloss:0.452458\teval-mlogloss:0.497149\n",
      "Stopping. Best iteration:\n",
      "[330]\ttrain-mlogloss:0.459927\teval-mlogloss:0.496714\n",
      "\n",
      "xgb now score is: [0.49671411228775153]\n",
      "[0]\ttrain-mlogloss:1.07418\teval-mlogloss:1.07429\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.504197\teval-mlogloss:0.519002\n",
      "[200]\ttrain-mlogloss:0.474539\teval-mlogloss:0.500735\n",
      "[300]\ttrain-mlogloss:0.462414\teval-mlogloss:0.499789\n",
      "Stopping. Best iteration:\n",
      "[280]\ttrain-mlogloss:0.464471\teval-mlogloss:0.499727\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814]\n",
      "[0]\ttrain-mlogloss:1.07385\teval-mlogloss:1.07431\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.503578\teval-mlogloss:0.522621\n",
      "[200]\ttrain-mlogloss:0.473817\teval-mlogloss:0.503675\n",
      "[300]\ttrain-mlogloss:0.46175\teval-mlogloss:0.502599\n",
      "Stopping. Best iteration:\n",
      "[299]\ttrain-mlogloss:0.461854\teval-mlogloss:0.502593\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008]\n",
      "[0]\ttrain-mlogloss:1.07397\teval-mlogloss:1.0741\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.50471\teval-mlogloss:0.518463\n",
      "[200]\ttrain-mlogloss:0.475162\teval-mlogloss:0.499052\n",
      "[300]\ttrain-mlogloss:0.463187\teval-mlogloss:0.49759\n",
      "[400]\ttrain-mlogloss:0.452215\teval-mlogloss:0.497643\n",
      "Stopping. Best iteration:\n",
      "[329]\ttrain-mlogloss:0.459936\teval-mlogloss:0.497506\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008, 0.49750612518114862]\n",
      "[0]\ttrain-mlogloss:1.074\teval-mlogloss:1.0743\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-mlogloss:0.504802\teval-mlogloss:0.518695\n",
      "[200]\ttrain-mlogloss:0.474831\teval-mlogloss:0.499178\n",
      "[300]\ttrain-mlogloss:0.462338\teval-mlogloss:0.498505\n",
      "Stopping. Best iteration:\n",
      "[263]\ttrain-mlogloss:0.466591\teval-mlogloss:0.4984\n",
      "\n",
      "xgb now score is: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008, 0.49750612518114862, 0.49839997481896325]\n",
      "xgb_score_list: [0.49671411228775153, 0.49972713562918814, 0.50259294017754008, 0.49750612518114862, 0.49839997481896325]\n",
      "xgb_score_mean: 0.498988057619\n"
     ]
    }
   ],
   "source": [
    "clf_level2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final submission with xgboost on stacked dataset, leads to a score of 0.50372 on Kaggle private leader board."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
