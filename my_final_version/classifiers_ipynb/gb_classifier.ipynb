{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble._gradient_boosting import predict_stage\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "import time\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function calculate term-document matrix with most frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_freq_vects(docs, max_feature=None, percent=None, token_pattern=u'(?u)\\b\\w\\w+\\b'):\n",
    "    vect = CountVectorizer(token_pattern=token_pattern)\n",
    "    feat_sparse = vect.fit_transform(docs.values.astype('U'))\n",
    "    freq_table = list(zip(vect.get_feature_names(), np.asarray(feat_sparse.sum(axis=0)).ravel()))\n",
    "    freq_table = pd.DataFrame(freq_table, columns=['feature', 'count']).sort_values('count', ascending=False)\n",
    "    if not max_feature:\n",
    "        if percent:\n",
    "            max_feature = int(percent * len(vect.get_feature_names()))\n",
    "        else:\n",
    "            max_feature = len(vect.get_feature_names())\n",
    "    feat_df = pd.DataFrame(feat_sparse.todense(), columns=vect.get_feature_names())\n",
    "    names = list(freq_table.feature[:max_feature])\n",
    "    return feat_df[names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function loading all engineered feature files, return dataframe of train and test, together with list of column names and listing_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print('Loading features files')\n",
    "    basic_feat = pd.read_json('../feat_input/basic_feat.json')\n",
    "    longtime_feat = pd.read_csv('../feat_input/longtime_feat.csv')\n",
    "    encoded_feat = pd.read_csv('../feat_input/feat_stats_encoding.csv')\n",
    "\n",
    "    # apply ordinal encoding to categorical feature\n",
    "    print('Ordinal encoding')\n",
    "    basic_feat.display_address = basic_feat.display_address.replace(r'\\r$', '', regex=True)\n",
    "    basic_feat.street_address = basic_feat.street_address.replace(r'\\r$', '', regex=True)\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "    for f in categorical:\n",
    "        if basic_feat[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(basic_feat[f].values))\n",
    "            basic_feat[f] = lbl.transform(list(basic_feat[f].values))\n",
    "\n",
    "    all_feat = basic_feat.merge(longtime_feat, on='listing_id')\n",
    "    all_feat = all_feat.merge(encoded_feat, on='listing_id')\n",
    "\n",
    "    print(\"Features document-term matrix\")\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punct = string.punctuation\n",
    "    punct = re.sub(\"'|-\", \"\", punct)\n",
    "    pattern = r\"[0-9]|[{}]\".format(punct)\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [re.sub(pattern, \"\", y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ['_'.join(['feature'] + y.split()) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ' '.join(x))\n",
    "    vect_df = most_freq_vects(all_feat['features'], max_feature=100, token_pattern=r\"[^ ]+\")\n",
    "    \n",
    "    all_feat = pd.concat([all_feat, vect_df], axis=1)\n",
    "    train = all_feat[all_feat.interest_level != -1].copy()\n",
    "    test = all_feat[all_feat.interest_level == -1].copy()\n",
    "    y_train=train[\"interest_level\"]\n",
    "\n",
    "    x_train = train.drop([\"interest_level\",\"features\"],axis=1)\n",
    "    x_test = test.drop([\"interest_level\",\"features\"],axis=1)\n",
    "\n",
    "    return x_train.as_matrix(), y_train.as_matrix(), x_test.as_matrix(), x_test.columns.values, x_test.listing_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with 5-fold cv. Output train and test multi-logloss as a measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gb_run_model(dtrain, dtest=None):\n",
    "    clf = GradientBoostingClassifier()\n",
    "    params = {'learning_rate': 0.02,\n",
    "              'n_estimators': 10000,\n",
    "              'subsample': 0.7,\n",
    "              'max_depth': 5,\n",
    "              'random_state': 36683,\n",
    "              'verbose': 1\n",
    "             }\n",
    "    clf.set_params(**params)\n",
    "    if dtest:\n",
    "        monitor = Monitor(X_valid=dtest[0].astype('float32'), y_valid=dtest[1], max_consecutive_decreases=20)\n",
    "        clf.fit(dtrain[0], dtrain[1], monitor=monitor)\n",
    "        y_train_pred, y_test_pred = clf.predict_proba(dtrain[0]), clf.predict_proba(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)\n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred = clf.predict_proba(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return clf, y_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a class `Monitor` is constructed to implement early_stopping when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Monitor():\n",
    "    \"\"\"\n",
    "    Monitor for early stopping in Gradient Boosting for classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_valid : array-like, shape = [n_samples, n_features]\n",
    "      Training vectors, where n_samples is the number of samples\n",
    "      and n_features is the number of features.\n",
    "    y_valid : array-like, shape = [n_samples]\n",
    "      Target values (integers in classification, real numbers in regression)\n",
    "      For classification, labels must correspond to classes.\n",
    "    max_consecutive_decreases : int, optional (default=5)\n",
    "      Early stopping criteria: when the number of consecutive iterations that\n",
    "      result in a worse performance on the validation set exceeds this value,\n",
    "      the training stops.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_valid, y_valid, max_consecutive_decreases=5):\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.max_consecutive_decreases = max_consecutive_decreases\n",
    "        self.losses = []\n",
    "\n",
    "\n",
    "    def __call__(self, i, clf, args):\n",
    "        if i == 0:\n",
    "            self.consecutive_decreases_ = 0\n",
    "            self.predictions = clf._init_decision_function(self.X_valid)\n",
    "\n",
    "        predict_stage(clf.estimators_, i, self.X_valid, clf.learning_rate, self.predictions)\n",
    "        self.losses.append(clf.loss_(self.y_valid, self.predictions))\n",
    "\n",
    "        if len(self.losses) >= 2 and self.losses[-1] > self.losses[-2]:\n",
    "            self.consecutive_decreases_ += 1\n",
    "        else:\n",
    "            self.consecutive_decreases_ = 0\n",
    "\n",
    "        if self.consecutive_decreases_ >= self.max_consecutive_decreases:\n",
    "            print(\"Too many consecutive decreases of loss on validation set\"\n",
    "                  \"({}): stopping early at iteration {}.\".format(self.consecutive_decreases_, i))\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function for training with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cv():\n",
    "    X_train, y_train_cls, X_test, _, _ = load_data()\n",
    "\n",
    "    cv_scores, n_folds = [], 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=816)\n",
    "    for i, (train_ind, val_ind) in enumerate(skf.split(X_train, y_train_cls)):\n",
    "        print(\"Running Fold\", i + 1, \"/\", n_folds)\n",
    "        start = time.time()\n",
    "        \n",
    "        train_x, val_x = X_train[train_ind, :], X_train[val_ind, :]\n",
    "        train_y, val_y = y_train_cls[train_ind], y_train_cls[val_ind]\n",
    "        clf, train_loss, val_loss = gb_run_model((train_x, train_y), (val_x, val_y))\n",
    "        cv_scores.append([train_loss, val_loss])\n",
    "        \n",
    "        print(\"train_loss: {0:.6f}, val_loss: {1:.6f}\".format(train_loss, val_loss))\n",
    "        \n",
    "        end = time.time()\n",
    "        m, s = divmod(end-start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))\n",
    "        \n",
    "    mean_train_loss = np.mean([cv_scores[i][0] for i in range(len(cv_scores))])\n",
    "    mean_val_loss = np.mean([cv_scores[i][1] for i in range(len(cv_scores))])\n",
    "    print(\"train_loss mean: {0:.6f}, val_loss mean: {1:.6f}\".format(mean_train_loss, mean_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Running Fold 1 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       25298.0981         131.4575          332.42m\n",
      "         2       25024.5890         127.2042          333.21m\n",
      "         3       24764.0831         122.5148          334.76m\n",
      "         4       24444.0237         117.0479          334.46m\n",
      "         5       24147.7868         112.6037          333.82m\n",
      "         6       23902.4284         107.7236          333.59m\n",
      "         7       23667.1135         105.0457          333.65m\n",
      "         8       23366.9989         100.9843          333.61m\n",
      "         9       23145.9277          97.5725          333.59m\n",
      "        10       22970.3695          93.0969          333.56m\n",
      "        20       21030.0554          66.7781          333.15m\n",
      "        30       19656.7061          48.9135          332.94m\n",
      "        40       18661.1013          37.5467          332.45m\n",
      "        50       17681.9865          27.5109          331.81m\n",
      "        60       17128.7844          21.3175          330.93m\n",
      "        70       16595.7294          15.2125          329.89m\n",
      "        80       16215.5948          12.5921          328.84m\n",
      "        90       15926.5275          10.2361          327.62m\n",
      "       100       15551.4022           7.5762          326.89m\n",
      "       200       13967.0596           1.8763          318.86m\n",
      "       300       13201.6957           0.5906          312.34m\n",
      "       400       12626.4950          -0.0372          306.48m\n",
      "       500       12105.0307           0.0498          301.34m\n",
      "       600       11731.6873          -0.4054          296.16m\n",
      "       700       11347.3896          -0.1867          291.74m\n",
      "       800       10954.2936          -0.1227          287.28m\n",
      "       900       10618.8829          -0.3223          282.72m\n",
      "      1000       10288.7917          -0.2354          278.54m\n",
      "      2000        7919.2063          -0.5196          243.66m\n",
      "      3000        6188.1376          -0.4403          214.72m\n",
      "      4000        4929.8004          -0.3777          195.44m\n",
      "Too many consecutive decreases of loss on validation set(20): stopping early at iteration 4876.\n",
      "train_loss: 0.147564, val_loss: 0.550625\n",
      "time elapsed: 2:46:59\n",
      "Running Fold 2 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       25301.4679         131.9259          350.94m\n",
      "         2       25001.5043         124.3313          346.76m\n",
      "         3       24736.1534         119.3630          354.30m\n",
      "         4       24472.5784         116.0084          350.18m\n",
      "         5       24199.7165         111.5163          347.55m\n",
      "         6       23941.9183         107.5357          345.28m\n",
      "         7       23651.6193         102.7541          347.87m\n",
      "         8       23437.4190          99.3520          350.21m\n",
      "         9       23208.0808          96.0337          349.18m\n",
      "        10       23034.1097          93.9571          347.71m\n",
      "        20       21073.7558          65.9293          356.07m\n",
      "        30       19688.0680          47.6372          351.45m\n",
      "        40       18665.9303          36.8176          349.38m\n",
      "        50       17918.0425          27.0586          345.37m\n",
      "        60       17198.2938          21.2745          349.39m\n",
      "        70       16734.2957          14.8462          361.95m\n",
      "        80       16279.8606          12.0394          397.53m\n",
      "        90       15955.8472          10.6733          391.52m\n",
      "       100       15725.2741           7.1848          393.41m\n",
      "       200       14225.4688           1.9024          406.02m\n",
      "       300       13235.2260           0.3118          371.05m\n",
      "       400       12753.0324           0.0041          348.74m\n",
      "       500       12227.3842          -0.1320          335.39m\n",
      "       600       11797.5728          -0.1395          324.48m\n",
      "       700       11398.5613          -0.2783          317.12m\n",
      "       800       11080.9157          -0.4319          310.45m\n",
      "       900       10637.5792          -0.2747          304.08m\n",
      "      1000       10410.0654          -0.3527          323.71m\n",
      "      2000        7939.8935          -0.4479          362.01m\n",
      "      3000        6207.3380          -0.4371          343.05m\n",
      "Too many consecutive decreases of loss on validation set(20): stopping early at iteration 3639.\n",
      "train_loss: 0.194940, val_loss: 0.519321\n",
      "time elapsed: 3:06:12\n",
      "Running Fold 3 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       25293.6273         130.9048          338.18m\n",
      "         2       24988.6724         125.7290          338.14m\n",
      "         3       24735.8450         120.5117          339.34m\n",
      "         4       24423.7317         115.7749          339.52m\n",
      "         5       24189.8756         112.3898          339.55m\n",
      "         6       23943.4215         108.2505          339.26m\n",
      "         7       23645.2573         102.8197          339.59m\n",
      "         8       23427.7862         101.2065          338.95m\n",
      "         9       23212.1316          96.8820          339.23m\n",
      "        10       22982.0093          93.7189          339.70m\n",
      "        20       21047.6633          66.4234          337.02m\n",
      "        30       19713.2341          48.7325          336.09m\n",
      "        40       18591.1678          35.0960          334.75m\n",
      "        50       17859.7603          26.5906          333.60m\n",
      "        60       17323.5156          22.3835          332.51m\n",
      "        70       16657.2750          15.2564          331.71m\n",
      "        80       16233.5233          13.2351          331.13m\n",
      "        90       15904.4003          11.0546          330.78m\n",
      "       100       15714.6681           8.2359          330.03m\n",
      "       200       14020.7036           1.1297          321.96m\n",
      "       300       13291.9934           0.5313          314.09m\n",
      "       400       12653.4443           0.4289          307.40m\n",
      "       500       12323.3368          -0.3009          301.67m\n",
      "       600       11708.8016          -0.3021          296.37m\n",
      "       700       11451.8134          -0.0516          291.33m\n",
      "       800       11051.6499          -0.4557          287.02m\n",
      "       900       10735.3768          -0.1005          282.89m\n",
      "      1000       10423.3323          -0.5089          279.02m\n",
      "      2000        7990.2706          -0.5026          245.04m\n",
      "      3000        6265.3445          -0.3430          213.81m\n",
      "      4000        5000.7757          -0.3388          183.17m\n",
      "Too many consecutive decreases of loss on validation set(20): stopping early at iteration 4994.\n",
      "train_loss: 0.146394, val_loss: 0.528812\n",
      "time elapsed: 2:33:18\n",
      "Running Fold 4 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       25321.6834         131.2441          335.94m\n",
      "         2       24981.2332         123.9565          336.46m\n",
      "         3       24708.0898         120.6653          337.13m\n",
      "         4       24396.2619         114.3400          337.13m\n",
      "         5       24115.7822         109.6237          337.21m\n",
      "         6       23917.8514         108.3735          337.14m\n",
      "         7       23665.4279         103.6693          337.22m\n",
      "         8       23442.5368         101.7383          337.35m\n",
      "         9       23152.9703          97.0459          336.97m\n",
      "        10       22953.7500          92.3779          336.54m\n",
      "        20       21030.7081          66.0152          335.80m\n",
      "        30       19560.8056          48.1188          334.39m\n",
      "        40       18616.8169          36.9204          332.94m\n",
      "        50       17779.9681          26.0047          331.67m\n",
      "        60       17168.7037          19.7647          330.69m\n",
      "        70       16693.4230          17.1169          330.02m\n",
      "        80       16333.9252          13.2665          329.01m\n",
      "        90       15935.6562          10.5850          328.25m\n",
      "       100       15607.6984           8.4119          327.47m\n",
      "       200       13942.4498           1.4905          320.04m\n",
      "       300       13206.3769           0.2974          312.75m\n",
      "       400       12677.8671           0.0150          306.12m\n",
      "       500       12199.9002          -0.5109          300.43m\n",
      "       600       11767.0584          -0.0310          295.07m\n",
      "       700       11368.4128          -0.3282          290.46m\n",
      "       800       10897.6036          -0.3819          286.34m\n",
      "       900       10717.7146          -0.2893          282.17m\n",
      "      1000       10381.2384          -0.6129          278.11m\n",
      "      2000        7847.6108          -0.4550          244.04m\n",
      "Too many consecutive decreases of loss on validation set(20): stopping early at iteration 2353.\n",
      "train_loss: 0.262591, val_loss: 0.516866\n",
      "time elapsed: 1:12:00\n",
      "Running Fold 5 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       25323.8064         131.4043          333.77m\n",
      "         2       25003.5722         126.0397          333.77m\n",
      "         3       24723.8559         121.3798          333.89m\n",
      "         4       24418.3418         117.5675          334.18m\n",
      "         5       24139.7681         112.4420          334.13m\n",
      "         6       23942.4294         108.0985          334.39m\n",
      "         7       23679.3764         103.3045          334.83m\n",
      "         8       23394.5921         100.5635          334.52m\n",
      "         9       23193.8573          96.1267          334.24m\n",
      "        10       22977.6285          93.2497          334.11m\n",
      "        20       21053.6562          65.7103          334.35m\n",
      "        30       19729.7678          48.5564          334.76m\n",
      "        40       18642.5651          35.1367          334.83m\n",
      "        50       17800.1827          26.6986          334.31m\n",
      "        60       17253.3684          20.8467          333.68m\n",
      "        70       16685.6469          16.7691          332.92m\n",
      "        80       16307.5721          11.9666          331.93m\n",
      "        90       16007.9062           9.9819          330.82m\n",
      "       100       15612.6422           7.5929          329.87m\n",
      "       200       14112.9764           1.3589          321.46m\n",
      "       300       13202.5249           0.5956          314.42m\n",
      "       400       12687.8390           0.3449          308.08m\n",
      "       500       12211.6169           0.3808          302.23m\n",
      "       600       11884.5525          -0.1460          296.70m\n",
      "       700       11427.9254          -0.0281          291.84m\n",
      "       800       10988.5193          -0.2487          287.66m\n",
      "       900       10713.7724          -0.2347          283.65m\n",
      "      1000       10437.1269          -0.5790          279.91m\n",
      "      2000        7948.8049          -0.3916          245.50m\n",
      "      3000        6259.6142          -0.3178          214.13m\n",
      "Too many consecutive decreases of loss on validation set(20): stopping early at iteration 3570.\n",
      "train_loss: 0.198208, val_loss: 0.516672\n",
      "time elapsed: 1:49:39\n",
      "train_loss mean: 0.189939, val_loss mean: 0.526459\n"
     ]
    }
   ],
   "source": [
    "train_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gradient boosting obtained the closest result to xgb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
