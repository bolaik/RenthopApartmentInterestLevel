{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Fold XGB CV\n",
    "\n",
    "### Problem of worse cv score with `json` file\n",
    "\n",
    "`basic_feat` imported from a pre-generated `json` file. The problem of worse cv score with `json` file has been resolved. And the reason is due to the fact that, in the underlying code:\n",
    "\n",
    "```python\n",
    "tr_sparse = vect_sparse[:train_num]\n",
    "te_sparse = vect_sparse[train_num:]\n",
    "```\n",
    "\n",
    "which requires in default the first `train_num` rows of `feat_all` should only contain training data. This is unfortunately not the case for the `json` file. So the following line is added to code:\n",
    "\n",
    "```python\n",
    "all_feat = all_feat.sort_values('interest_level', ascending=False).reset_index()\n",
    "```\n",
    "\n",
    "### Added custom function `most_freq_vects`\n",
    "\n",
    "Select most frequent terms in features, and the subsequent document-term-matrix\n",
    "\n",
    "### Pre-clean the features column data\n",
    "\n",
    "remove `[0-9]` and punctuations except `['|-]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_freq_vects(docs, max_feature=None, percent=None, token_pattern=u'(?u)\\b\\w\\w+\\b'):\n",
    "    vect = CountVectorizer(token_pattern=token_pattern)\n",
    "    feat_sparse = vect.fit_transform(docs.values.astype('U'))\n",
    "    freq_table = list(zip(vect.get_feature_names(), np.asarray(feat_sparse.sum(axis=0)).ravel()))\n",
    "    freq_table = pd.DataFrame(freq_table, columns=['feature', 'count']).sort_values('count', ascending=False)\n",
    "    if not max_feature:\n",
    "        if percent:\n",
    "            max_feature = int(percent * len(vect.get_feature_names()))\n",
    "        else:\n",
    "            max_feature = len(vect.get_feature_names())\n",
    "    feat_df = pd.DataFrame(feat_sparse.todense(), columns=vect.get_feature_names())\n",
    "    names = list(freq_table.feature[:max_feature])\n",
    "    return names, csr_matrix(feat_df[names].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_data_prep():\n",
    "    basic_feat = pd.read_json('../feat_input/basic_feat.json')\n",
    "    longtime_feat = pd.read_csv('../feat_input/longtime_feat.csv')\n",
    "    encoded_feat = pd.read_csv('../feat_input/feat_stats_encoding.csv')\n",
    "    print('Loading features finished')\n",
    "\n",
    "    # apply ordinal encoding to categorical feature\n",
    "    basic_feat.display_address = basic_feat.display_address.replace(r'\\r$', '', regex=True)\n",
    "    basic_feat.street_address = basic_feat.street_address.replace(r'\\r$', '', regex=True)\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "    for f in categorical:\n",
    "        if basic_feat[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(basic_feat[f].values))\n",
    "            basic_feat[f] = lbl.transform(list(basic_feat[f].values))\n",
    "    print('Ordinal encoding finished')\n",
    "\n",
    "    all_feat = basic_feat.merge(longtime_feat, on='listing_id')\n",
    "    all_feat = all_feat.merge(encoded_feat, on='listing_id')\n",
    "\n",
    "    all_feat = all_feat.sort_values('interest_level', ascending=False).reset_index()\n",
    "    train = all_feat[all_feat.interest_level != -1].copy()\n",
    "    test = all_feat[all_feat.interest_level == -1].copy()\n",
    "    y_train=train[\"interest_level\"]\n",
    "\n",
    "    train_num=train.shape[0]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    import string\n",
    "    punct = string.punctuation\n",
    "    punct = re.sub(\"'|-\", \"\", punct)\n",
    "    pattern = r\"[0-9]|[{}]\".format(punct)\n",
    "    \n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [re.sub(pattern, \"\", y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ['_'.join(['feature'] + y.split()) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    vect_names, vect_sparse = most_freq_vects(all_feat['features'], max_feature=100, token_pattern=r\"[^ ]+\")\n",
    "    print(vect_names)\n",
    "    \n",
    "    tr_sparse = vect_sparse[:train_num]\n",
    "    te_sparse = vect_sparse[train_num:]\n",
    "    print(\"Document-term matrix finished\")\n",
    "\n",
    "    x_train = train.drop([\"interest_level\",\"features\",\"index\"],axis=1)\n",
    "    x_test = test.drop([\"interest_level\",\"features\",\"index\"],axis=1)\n",
    "\n",
    "    x_train = sparse.hstack([x_train.astype(float),tr_sparse.astype(float)]).tocsr()\n",
    "    x_test = sparse.hstack([x_test.astype(float),te_sparse.astype(float)]).tocsr()\n",
    "\n",
    "    return x_train, y_train, x_test, test.listing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv(dtrain, num_rounds = 50000, early_stop_rounds=250):\n",
    "    print('Start xgboost cross-validation')\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective': 'multi:softprob',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'gamma': 1,\n",
    "              'min_child_weight': 1.5,\n",
    "              'max_depth': 5,\n",
    "              'lambda': 10,\n",
    "              'subsample': 0.7,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'colsample_bylevel': 0.7,\n",
    "              'eta': 0.03,\n",
    "              'tree_method': 'exact',\n",
    "              'seed': 36683,\n",
    "              'nthread': 4,\n",
    "              'num_class': 3,\n",
    "              'silent': 1\n",
    "              }\n",
    "    xgb2cv = xgb.cv(params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=num_rounds,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    verbose_eval=50,\n",
    "                    early_stopping_rounds=early_stop_rounds)\n",
    "\n",
    "    return xgb2cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features finished\n",
      "Ordinal encoding finished\n",
      "['feature_elev', 'feature_cats_allow', 'feature_hardwood_floor', 'feature_dogs_allow', 'feature_doorman', 'feature_dishwash', 'feature_laundry_in_build', 'feature_no_fe', 'feature_fitness_cent', 'feature_laundry_in_unit', 'feature_pre-war', 'feature_roof_deck', 'feature_outdoor_spac', 'feature_dining_room', 'feature_high_speed_internet', 'feature_balconi', 'feature_swimming_pool', 'feature_new_construct', 'feature_terrac', 'feature_exclus', 'feature_loft', 'feature_gardenpatio', 'feature_prewar', 'feature_wheelchair_access', 'feature_common_outdoor_spac', 'feature_hardwood', 'feature_simplex', 'feature_fireplac', 'feature_high_ceil', 'feature_lowris', 'feature_garag', 'feature_reduced_fe', 'feature_laundry_room', 'feature_furnish', 'feature_multi-level', 'feature_private_outdoor_spac', 'feature_parking_spac', 'feature_publicoutdoor', 'feature_roof-deck', 'feature_live_in_sup', 'feature_renov', 'feature_pool', 'feature_on-site_laundri', 'feature_laundri', 'feature_green_build', 'feature_storag', 'feature_stainless_steel_appli', 'feature_concierg', 'feature_washer_in_unit', 'feature_dryer_in_unit', 'feature_newly_renov', 'feature_on-site_garag', 'feature_light', 'feature_patio', 'feature_washerdry', 'feature_walk_in_closet', 'feature_live-in_superintend', 'feature_gymfit', 'feature_exposed_brick', 'feature_granite_kitchen', 'feature_bike_room', 'feature_pets_on_approv', 'feature_garden', 'feature_marble_bath', 'feature_valet', 'feature_subway', 'feature_residents_loung', 'feature_eat_in_kitchen', 'feature_central_ac', 'feature_live-in_sup', 'feature_full-time_doorman', 'feature_common_parkinggarag', 'feature_wifi_access', 'feature_park', 'feature_highris', 'feature_loung', 'feature_short_term_allow', 'feature_childrens_playroom', 'feature_no_pet', 'feature_duplex', 'feature_actual_apt_photo', 'feature_view', 'feature_luxury_build', 'feature_gym', 'feature_common_roof_deck', 'feature_residents_garden', 'feature_outdoor_area', 'feature_roofdeck', 'feature_indoor_pool', \"feature_children's_playroom\", 'feature_virtual_doorman', 'feature_livework', 'feature_building-common-outdoor-spac', 'feature_washer_dry', 'feature_microwav', 'feature_sauna', 'feature_valet_park', 'feature_air_condit', 'feature_lounge_room', 'feature_sublet']\n",
      "Document-term matrix finished\n",
      "Start xgboost cross-validation\n",
      "[0]\ttrain-mlogloss:1.07859+6.43223e-05\ttest-mlogloss:1.07884+0.000121015\n",
      "[50]\ttrain-mlogloss:0.659432+0.000761891\ttest-mlogloss:0.669132+0.00225996\n",
      "[100]\ttrain-mlogloss:0.568857+0.000822665\ttest-mlogloss:0.585854+0.00317809\n",
      "[150]\ttrain-mlogloss:0.534466+0.000939812\ttest-mlogloss:0.557987+0.00380664\n",
      "[200]\ttrain-mlogloss:0.514537+0.00102485\ttest-mlogloss:0.54432+0.00412779\n",
      "[250]\ttrain-mlogloss:0.499606+0.00136153\ttest-mlogloss:0.535833+0.00430591\n",
      "[300]\ttrain-mlogloss:0.487313+0.00125452\ttest-mlogloss:0.529799+0.00459405\n",
      "[350]\ttrain-mlogloss:0.47694+0.00130426\ttest-mlogloss:0.525597+0.0046723\n",
      "[400]\ttrain-mlogloss:0.467696+0.00130452\ttest-mlogloss:0.52237+0.00475276\n",
      "[450]\ttrain-mlogloss:0.459226+0.00133491\ttest-mlogloss:0.51986+0.00482994\n",
      "[500]\ttrain-mlogloss:0.451237+0.00149174\ttest-mlogloss:0.517612+0.00494847\n",
      "[550]\ttrain-mlogloss:0.44385+0.00144033\ttest-mlogloss:0.515817+0.00506989\n",
      "[600]\ttrain-mlogloss:0.436739+0.00138445\ttest-mlogloss:0.514282+0.00518341\n",
      "[650]\ttrain-mlogloss:0.429907+0.00139367\ttest-mlogloss:0.513071+0.00539763\n",
      "[700]\ttrain-mlogloss:0.423577+0.00142137\ttest-mlogloss:0.511992+0.00544467\n",
      "[750]\ttrain-mlogloss:0.417398+0.00151419\ttest-mlogloss:0.511064+0.00541102\n",
      "[800]\ttrain-mlogloss:0.411529+0.00154716\ttest-mlogloss:0.510246+0.00552161\n",
      "[850]\ttrain-mlogloss:0.405848+0.00158952\ttest-mlogloss:0.509556+0.00556677\n",
      "[900]\ttrain-mlogloss:0.400292+0.00170856\ttest-mlogloss:0.509022+0.00567048\n",
      "[950]\ttrain-mlogloss:0.394866+0.00173139\ttest-mlogloss:0.508612+0.005715\n",
      "[1000]\ttrain-mlogloss:0.389634+0.00176962\ttest-mlogloss:0.508157+0.00582317\n",
      "[1050]\ttrain-mlogloss:0.384505+0.00176698\ttest-mlogloss:0.507723+0.00597768\n",
      "[1100]\ttrain-mlogloss:0.379519+0.00181994\ttest-mlogloss:0.507445+0.00611936\n",
      "[1150]\ttrain-mlogloss:0.374709+0.00190866\ttest-mlogloss:0.507161+0.00620613\n",
      "[1200]\ttrain-mlogloss:0.36998+0.00191777\ttest-mlogloss:0.506929+0.00627962\n",
      "[1250]\ttrain-mlogloss:0.365519+0.00198255\ttest-mlogloss:0.50675+0.00631454\n",
      "[1300]\ttrain-mlogloss:0.360899+0.00199602\ttest-mlogloss:0.506646+0.00626991\n",
      "[1350]\ttrain-mlogloss:0.356464+0.00195948\ttest-mlogloss:0.506498+0.00634392\n",
      "[1400]\ttrain-mlogloss:0.352166+0.00195638\ttest-mlogloss:0.50626+0.00638077\n",
      "[1450]\ttrain-mlogloss:0.348019+0.00195154\ttest-mlogloss:0.506225+0.00643549\n",
      "[1500]\ttrain-mlogloss:0.34384+0.00193616\ttest-mlogloss:0.50619+0.00647149\n",
      "[1550]\ttrain-mlogloss:0.339896+0.00193457\ttest-mlogloss:0.506161+0.00658857\n",
      "[1600]\ttrain-mlogloss:0.335977+0.00187648\ttest-mlogloss:0.506169+0.00670243\n",
      "[1650]\ttrain-mlogloss:0.332141+0.00191415\ttest-mlogloss:0.506192+0.00677228\n",
      "[1700]\ttrain-mlogloss:0.328389+0.00199634\ttest-mlogloss:0.506201+0.00682164\n",
      "[1750]\ttrain-mlogloss:0.324718+0.00201518\ttest-mlogloss:0.506264+0.00689616\n",
      "[1800]\ttrain-mlogloss:0.321072+0.0020485\ttest-mlogloss:0.50633+0.00698011\n",
      "[1850]\ttrain-mlogloss:0.317532+0.00199977\ttest-mlogloss:0.506434+0.0069986\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, listing_id = xgb_data_prep()\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# cross-validation and training\n",
    "xgb_cv_hist = xgb_cv(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
