{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD classifier for linear models\n",
    "\n",
    "This estimator implements regularized linear models (SVM, logistic regression, a.o.) with stochastic gradient descent (SGD) learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re, string, time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom functions for loading and preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_freq_vects(docs, max_feature=None, percent=None, token_pattern=u'(?u)\\b\\w\\w+\\b'):\n",
    "    vect = CountVectorizer(token_pattern=token_pattern)\n",
    "    feat_sparse = vect.fit_transform(docs.values.astype('U'))\n",
    "    freq_table = list(zip(vect.get_feature_names(), np.asarray(feat_sparse.sum(axis=0)).ravel()))\n",
    "    freq_table = pd.DataFrame(freq_table, columns=['feature', 'count']).sort_values('count', ascending=False)\n",
    "    if not max_feature:\n",
    "        if percent:\n",
    "            max_feature = int(percent * len(vect.get_feature_names()))\n",
    "        else:\n",
    "            max_feature = len(vect.get_feature_names())\n",
    "    feat_df = pd.DataFrame(feat_sparse.todense(), columns=vect.get_feature_names())\n",
    "    names = list(freq_table.feature[:max_feature])\n",
    "    return feat_df[names]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    print('Loading features files')\n",
    "    basic_feat = pd.read_json('../feat_input/basic_feat.json')\n",
    "    longtime_feat = pd.read_csv('../feat_input/longtime_feat.csv')\n",
    "    encoded_feat = pd.read_csv('../feat_input/feat_stats_encoding.csv')\n",
    "\n",
    "    # apply ordinal encoding to categorical feature\n",
    "    print('Ordinal encoding')\n",
    "    basic_feat.display_address = basic_feat.display_address.replace(r'\\r$', '', regex=True)\n",
    "    basic_feat.street_address = basic_feat.street_address.replace(r'\\r$', '', regex=True)\n",
    "    categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "    for f in categorical:\n",
    "        if basic_feat[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(basic_feat[f].values))\n",
    "            basic_feat[f] = lbl.transform(list(basic_feat[f].values))\n",
    "\n",
    "    all_feat = basic_feat.merge(longtime_feat, on='listing_id')\n",
    "    all_feat = all_feat.merge(encoded_feat, on='listing_id')\n",
    "\n",
    "    print(\"Features document-term matrix\")\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    punct = string.punctuation\n",
    "    punct = re.sub(\"'|-\", \"\", punct)\n",
    "    pattern = r\"[0-9]|[{}]\".format(punct)\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [re.sub(pattern, \"\", y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ['_'.join(['feature'] + y.split()) for y in x])\n",
    "    all_feat['features'] = all_feat['features'].apply(lambda x: ' '.join(x))\n",
    "    vect_df = most_freq_vects(all_feat['features'], max_feature=100, token_pattern=r\"[^ ]+\")\n",
    "    \n",
    "    all_feat = pd.concat([all_feat, vect_df], axis=1)\n",
    "    train = all_feat[all_feat.interest_level != -1].copy()\n",
    "    test = all_feat[all_feat.interest_level == -1].copy()\n",
    "    y_train=train[\"interest_level\"]\n",
    "\n",
    "    x_train = train.drop([\"interest_level\",\"features\"],axis=1)\n",
    "    x_test = test.drop([\"interest_level\",\"features\"],axis=1)\n",
    "\n",
    "    return x_train, y_train, x_test, x_test.columns.values, x_test.listing_id\n",
    "\n",
    "\n",
    "def _preprocess(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 5-fold cv to train the model, **with default parameter setting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(dtrain, dtest=None):\n",
    "    clf = SGDClassifier()\n",
    "    params = {'loss': 'log',\n",
    "              'alpha': 1e-4,\n",
    "              'n_jobs': -1,\n",
    "              'verbose': 1,\n",
    "              'random_state': 36883\n",
    "              }\n",
    "    clf.set_params(**params)\n",
    "    if dtest:\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred, y_test_pred = clf.predict_proba(dtrain[0]), clf.predict_proba(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)\n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred = clf.predict_proba(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return clf, y_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cv(preprocess='linear'):\n",
    "    X_train, y_train_cls, X_test, _, _ = load_data()\n",
    "    if preprocess == 'log':\n",
    "        X_train, X_test = _preprocess_log(X_train, X_test)\n",
    "    else:\n",
    "        X_train, X_test = _preprocess(X_train, X_test)\n",
    "\n",
    "    cv_scores, n_folds = [], 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=816)\n",
    "    for i, (train_ind, val_ind) in enumerate(skf.split(X_train, y_train_cls)):\n",
    "        print(\"Running Fold\", i + 1, \"/\", n_folds)\n",
    "        start = time.time()\n",
    "        \n",
    "        train_x, val_x = X_train.iloc[train_ind, :], X_train.iloc[val_ind, :]\n",
    "        train_y, val_y = y_train_cls.iloc[train_ind], y_train_cls.iloc[val_ind]\n",
    "        clf, train_loss, val_loss = run_model((train_x, train_y), (val_x, val_y))\n",
    "        cv_scores.append([train_loss, val_loss])\n",
    "        \n",
    "        print(\"train_loss: {0:.6f}, val_loss: {1:.6f}\".format(train_loss, val_loss), end=\"\\t\")\n",
    "        \n",
    "        end = time.time()\n",
    "        m, s = divmod(end-start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))\n",
    "        \n",
    "    mean_train_loss = np.mean([cv_scores[i][0] for i in range(len(cv_scores))])\n",
    "    mean_val_loss = np.mean([cv_scores[i][1] for i in range(len(cv_scores))])\n",
    "    print(\"train_loss mean: {0:.6f}, val_loss mean: {1:.6f}\".format(mean_train_loss, mean_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logloss, which is equivalent to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features files\n",
      "Ordinal encoding\n",
      "Features document-term matrix\n",
      "Running Fold 1 / 5\n",
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "Norm: 51.51, NNZs: 192, Bias: -23.384622, T: 39481, Avg. loss: 23.803967Norm: 59.25, NNZs: 192, Bias: 19.892870, T: 39481, Avg. loss: 21.247254\n",
      "Total training time: 0.03 seconds.\n",
      "\n",
      "Total training time: 0.03 seconds.-- Epoch 2\n",
      "\n",
      "Norm: 102.54, NNZs: 192, Bias: -149.975169, T: 39481, Avg. loss: 13.192500-- Epoch 2\n",
      "Total training time: 0.03 seconds.\n",
      "\n",
      "-- Epoch 2\n",
      "Norm: 34.40, NNZs: 192, Bias: 11.860152, T: 78962, Avg. loss: 12.803241Norm: 27.99, NNZs: 192, Bias: -12.219804, T: 78962, Avg. loss: 14.091982\n",
      "Total training time: 0.06 seconds.\n",
      "\n",
      "-- Epoch 3Total training time: 0.06 seconds.\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 56.24, NNZs: 192, Bias: -53.047952, T: 78962, Avg. loss: 7.984804\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 20.40, NNZs: 192, Bias: 7.635966, T: 118443, Avg. loss: 9.397871\n",
      "Total training time: 0.09 seconds.Norm: 20.90, NNZs: 192, Bias: -8.263069, T: 118443, Avg. loss: 10.251257\n",
      "-- Epoch 4\n",
      "\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.80, NNZs: 192, Bias: -26.807070, T: 118443, Avg. loss: 5.742627\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.30, NNZs: 192, Bias: 4.980842, T: 157924, Avg. loss: 7.505891Norm: 15.68, NNZs: 192, Bias: -7.542707, T: 157924, Avg. loss: 8.161368\n",
      "Total training time: 0.12 seconds.\n",
      "\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "-- Epoch 5\n",
      "Norm: 26.11, NNZs: 192, Bias: -17.382610, T: 157924, Avg. loss: 4.520299\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 15.00, NNZs: 192, Bias: -5.992111, T: 197405, Avg. loss: 6.838398\n",
      "Total training time: 0.15 seconds.\n",
      "Norm: 12.50, NNZs: 192, Bias: 4.473408, T: 197405, Avg. loss: 6.301367\n",
      "Total training time: 0.16 seconds.\n",
      "Norm: 26.10, NNZs: 192, Bias: -13.694108, T: 197405, Avg. loss: 3.743551\n",
      "Total training time: 0.16 seconds.\n",
      "train_loss: 1.477533, val_loss: 1.521745\ttime elapsed: 0:00:00\n",
      "Running Fold 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 73.87, NNZs: 192, Bias: -24.839262, T: 39481, Avg. loss: 24.180336\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 55.45, NNZs: 192, Bias: 19.383894, T: 39481, Avg. loss: 23.429065\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 106.01, NNZs: 192, Bias: -156.082837, T: 39481, Avg. loss: 13.599829\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 35.94, NNZs: 192, Bias: -12.459367, T: 78962, Avg. loss: 14.295706\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 38.89, NNZs: 192, Bias: 8.483545, T: 78962, Avg. loss: 13.899961Norm: 58.32, NNZs: 192, Bias: -56.796536, T: 78962, Avg. loss: 8.288049\n",
      "\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3Total training time: 0.08 seconds.\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 21.53, NNZs: 192, Bias: -8.606911, T: 118443, Avg. loss: 10.408919\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 23.66, NNZs: 192, Bias: 7.265872, T: 118443, Avg. loss: 10.114465Norm: 36.70, NNZs: 192, Bias: -27.941497, T: 118443, Avg. loss: 5.974102\n",
      "Total training time: 0.11 seconds.\n",
      "\n",
      "-- Epoch 4Total training time: 0.12 seconds.\n",
      "\n",
      "-- Epoch 4\n",
      "Norm: 15.02, NNZs: 192, Bias: -6.314382, T: 157924, Avg. loss: 8.276507\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.24, NNZs: 192, Bias: 5.716955, T: 157924, Avg. loss: 8.050720Norm: 29.97, NNZs: 192, Bias: -18.258787, T: 157924, Avg. loss: 4.696011\n",
      "Total training time: 0.15 seconds.\n",
      "\n",
      "-- Epoch 5Total training time: 0.14 seconds.\n",
      "\n",
      "-- Epoch 5\n",
      "Norm: 12.09, NNZs: 192, Bias: -5.422047, T: 197405, Avg. loss: 6.920455\n",
      "Total training time: 0.18 seconds.\n",
      "Norm: 15.94, NNZs: 192, Bias: 4.829443, T: 197405, Avg. loss: 6.736436Norm: 22.82, NNZs: 192, Bias: -13.998029, T: 197405, Avg. loss: 3.889203\n",
      "\n",
      "Total training time: 0.18 seconds.Total training time: 0.17 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.598521, val_loss: 1.580136\ttime elapsed: 0:00:00\n",
      "Running Fold 3 / 5\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 58.45, NNZs: 192, Bias: 21.099649, T: 39481, Avg. loss: 21.892711\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 64.95, NNZs: 192, Bias: -24.283633, T: 39481, Avg. loss: 23.976645\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 87.88, NNZs: 192, Bias: -122.282128, T: 39481, Avg. loss: 11.716073\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 36.72, NNZs: 192, Bias: 10.100100, T: 78962, Avg. loss: 13.157032\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 48.39, NNZs: 192, Bias: -45.826512, T: 78962, Avg. loss: 7.089791\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 32.52, NNZs: 192, Bias: -12.099215, T: 78962, Avg. loss: 14.160449\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 22.63, NNZs: 192, Bias: 7.087966, T: 118443, Avg. loss: 9.629450\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 30.19, NNZs: 192, Bias: -23.320150, T: 118443, Avg. loss: 5.109204\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.75, NNZs: 192, Bias: 5.376395, T: 157924, Avg. loss: 7.677165\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 21.40, NNZs: 192, Bias: -8.244082, T: 118443, Avg. loss: 10.315414\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 21.92, NNZs: 192, Bias: -16.041203, T: 157924, Avg. loss: 4.027186\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13.99, NNZs: 192, Bias: 4.569320, T: 197405, Avg. loss: 6.427523\n",
      "Total training time: 0.20 seconds.\n",
      "Norm: 14.43, NNZs: 192, Bias: -6.425782, T: 157924, Avg. loss: 8.206169\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 23.43, NNZs: 192, Bias: -13.002638, T: 197405, Avg. loss: 3.344860\n",
      "Total training time: 0.23 seconds.\n",
      "Norm: 11.63, NNZs: 192, Bias: -5.569861, T: 197405, Avg. loss: 6.867151\n",
      "Total training time: 0.23 seconds.\n",
      "train_loss: 1.503052, val_loss: 1.565063\ttime elapsed: 0:00:00\n",
      "Running Fold 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "Norm: 73.30, NNZs: 192, Bias: -25.260119, T: 39481, Avg. loss: 23.672788\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 75.59, NNZs: 192, Bias: 21.369607, T: 39481, Avg. loss: 23.352540\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 100.52, NNZs: 192, Bias: -148.886450, T: 39481, Avg. loss: 12.906742\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 54.97, NNZs: 192, Bias: -53.719352, T: 78962, Avg. loss: 7.824650\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 38.17, NNZs: 192, Bias: 9.848600, T: 78962, Avg. loss: 13.854520\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 34.97, NNZs: 192, Bias: -12.505304, T: 78962, Avg. loss: 14.099451\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 35.75, NNZs: 192, Bias: -25.892027, T: 118443, Avg. loss: 5.627073\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 24.64, NNZs: 192, Bias: 8.738233, T: 118443, Avg. loss: 10.060851\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 22.05, NNZs: 192, Bias: -9.419804, T: 118443, Avg. loss: 10.275356\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 26.59, NNZs: 192, Bias: -16.913548, T: 157924, Avg. loss: 4.418530\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.26, NNZs: 192, Bias: 5.266856, T: 157924, Avg. loss: 8.002381\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.23, NNZs: 192, Bias: -6.030453, T: 157924, Avg. loss: 8.184719\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.36, NNZs: 192, Bias: 4.926809, T: 197405, Avg. loss: 6.690626\n",
      "Total training time: 0.24 seconds.\n",
      "Norm: 25.64, NNZs: 192, Bias: -13.905662, T: 197405, Avg. loss: 3.659255\n",
      "Total training time: 0.23 seconds.\n",
      "Norm: 13.47, NNZs: 192, Bias: -5.398741, T: 197405, Avg. loss: 6.850470\n",
      "Total training time: 0.23 seconds.\n",
      "train_loss: 1.593232, val_loss: 1.635632\ttime elapsed: 0:00:00\n",
      "Running Fold 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 79.87, NNZs: 192, Bias: -24.593413, T: 39484, Avg. loss: 23.343858\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 79.54, NNZs: 192, Bias: 18.486360, T: 39484, Avg. loss: 22.053744\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 94.80, NNZs: 192, Bias: -144.507312, T: 39484, Avg. loss: 12.476474\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 33.41, NNZs: 192, Bias: -12.673495, T: 78968, Avg. loss: 13.875275\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 37.84, NNZs: 192, Bias: 9.997301, T: 78968, Avg. loss: 13.208971\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 53.40, NNZs: 192, Bias: -49.088796, T: 78968, Avg. loss: 7.572454\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23.09, NNZs: 192, Bias: 8.839106, T: 118452, Avg. loss: 9.654947Norm: 21.87, NNZs: 192, Bias: -8.464138, T: 118452, Avg. loss: 10.114812\n",
      "\n",
      "Total training time: 0.11 seconds.Total training time: 0.12 seconds.\n",
      "\n",
      "-- Epoch 4-- Epoch 4\n",
      "\n",
      "Norm: 35.06, NNZs: 192, Bias: -25.595764, T: 118452, Avg. loss: 5.467711\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.81, NNZs: 192, Bias: 5.381008, T: 157936, Avg. loss: 7.698365Norm: 16.05, NNZs: 192, Bias: -5.933637, T: 157936, Avg. loss: 8.050177\n",
      "Total training time: 0.15 seconds.\n",
      "\n",
      "Total training time: 0.15 seconds.-- Epoch 5\n",
      "\n",
      "-- Epoch 5\n",
      "Norm: 25.30, NNZs: 192, Bias: -17.695091, T: 157936, Avg. loss: 4.305271\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13.86, NNZs: 192, Bias: 4.946550, T: 197420, Avg. loss: 6.445115Norm: 13.96, NNZs: 192, Bias: -5.715611, T: 197420, Avg. loss: 6.735337\n",
      "\n",
      "Total training time: 0.18 seconds.Total training time: 0.18 seconds.\n",
      "\n",
      "Norm: 20.62, NNZs: 192, Bias: -13.757441, T: 197420, Avg. loss: 3.573350\n",
      "Total training time: 0.18 seconds.\n",
      "train_loss: 1.401978, val_loss: 1.512059\ttime elapsed: 0:00:00\n",
      "train_loss mean: 1.514863, val_loss mean: 1.562927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/bolaik/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "train_cv(preprocess='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven't understand in detail about this method, the performance here is simply poor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
