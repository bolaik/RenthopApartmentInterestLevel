mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
" ([a-z]){1,2} |^([a-z]){1,2} | ([a-z]){1,2}$", " ")
mycorpus <- tm_map(mycorpus, toString,
" [a-z] |^[a-z] | [a-z]$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# document-term matrix of bigrams
#BigramTokenizer <- function(x)
#      unlist(lapply(ngrams(words(x), 2), paste, collapse = "_"), use.names = FALSE)
unigram_dtm <- DocumentTermMatrix(mycorpus)
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
head(t1_dscpt, 20)
gsub(" [a-z] ", " ", "nearbi g train")
gsub(" ([a-z]){1,2} |^([a-z]){1,2} | ([a-z]){1,2}$", " ", "nearbi g train")
gsub(" ([a-z]){1,2} |^([a-z]){1,2} | ([a-z]){1,2}$", " ", "flex bedroom full pressur wall look perfect apart midtown east sutton place come check beauti apart prime locat mid st ave elev build hour doorman laundri room bike room larg live space king bedroom beauti larg kitchen stainless steel applianc includ dishwash stun modern bathroom ampl amount closet space throughout entir apart enough space fit everyth need apart short distanc shop restaur public transport train don miss great apart act quick call text edan schedul privat view today websit redact")
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
" ([a-z]){1,3} |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# document-term matrix of bigrams
#BigramTokenizer <- function(x)
#      unlist(lapply(ngrams(words(x), 2), paste, collapse = "_"), use.names = FALSE)
unigram_dtm <- DocumentTermMatrix(mycorpus)
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
head(t1_dscpt, 10)
gsub(" ([a-z]){1,3} |^([a-z]){1,3} | ([a-z]){1,3}$", " ", "flex bedroom full pressur wall look perfect apart midtown east sutton place come check beauti apart prime locat s ave elev build hour doorman laundri room bike room larg live space king bedroom beauti larg kitchen stainless steel applianc includ dishwash stun modern bathroom ampl amount closet space throughout entir apart enough space everyth need apart short distanc shop restaur public transport m train t miss great apart quick call text edan schedul privat view today websit redact")
gsub(" ([a-z]){1,3} |^([a-z]){1,3} | ([a-z]){1,3}$", " ", "locat s ave elev shop restaur public transport m train t miss")
gsub("[ ([a-z]){1,3} ]+|^([a-z]){1,3} | ([a-z]){1,3}$", " ", "locat s ave elev shop restaur public transport m train t miss")
gsub("( ([a-z]){1,3} )+|^([a-z]){1,3} | ([a-z]){1,3}$", " ", "locat s ave elev shop restaur public transport m train t miss")
gsub("( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ", "locat s ave elev shop restaur public transport m train t miss")
gsub("[ ([a-z]){1,3}]+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ", "locat s ave elev shop restaur public transport m train t miss")
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# document-term matrix of bigrams
#BigramTokenizer <- function(x)
#      unlist(lapply(ngrams(words(x), 2), paste, collapse = "_"), use.names = FALSE)
unigram_dtm <- DocumentTermMatrix(mycorpus)
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
head(t1_dscpt, 20)
library(tau) #package to create N-Grams
ngram <- function(docs, n) { textcnt(docs, method = "string",n=as.integer(n),
split = "[ ]+",decreasing=T)
}
unigram_tau <- ngram(t1_dscpt$text, 1)
unigram_wf<-data.table(uni = names(unigram_tau), counts = unclass(unigram_tau))
rm(unigram_tau)
head(unigram_wf)
head(unigram_wf, 20)
library(tau) #package to create N-Grams
ngram <- function(docs, n) { textcnt(docs, method = "string",n=as.integer(n),
split = "[ ]+",decreasing=T)
}
bigram_tau <- ngram(t1_dscpt$text, 2)
bigram_wf<-data.table(bigram = names(bigram_tau), counts = unclass(bigram_tau))
rm(bigram_tau)
head(bigram_wf, 20)
library(tau) #package to create N-Grams
ngram <- function(docs, n) { textcnt(docs, method = "string",n=as.integer(n),
split = "[ ]+",decreasing=T)
}
trigram_tau <- ngram(t1_dscpt$text, 3)
trigram_wf<-data.table(trigram = names(trigram_tau), counts = unclass(trigram_tau))
rm(trigram_tau)
head(trigram_wf, 20)
head(trigram_wf, 100)
sum(trigram_wf$counts>100)
library(tau)
# custom function to create N-Grams from vector texts
ngram <- function(docs, n) {textcnt(docs, method = "string",n=as.integer(n),
split = "[ ]+",decreasing=T)}
# extract unigram from description
unigramDT <- sapply(1:nrow(t1_dscpt), function(row) {
ug <- ngram(t1_dscpt$text[row], 1)
data.table(listing_id = t1_dscpt$listing_id[row],
unigram = names(ug), counts = unclass(ug))
})
dim(uq)
dim(ug)
ug <- ngram(t1_dscpt$text[1], 1)
dim(ug)
summary(ug)
ug
library(tau)
# custom function to create N-Grams from vector texts
ngram <- function(docs, n) {textcnt(docs, method = "string",n=as.integer(n),
split = "[ ]+",decreasing=T)}
# extract unigram from description
unigramDT <- sapply(1:nrow(t1_dscpt), function(row) {
ug <- ngram(t1_dscpt$text[row], 1)
data.table(listing_id = rep(t1_dscpt$listing_id[row], length(names(ug)))
, unigram = names(ug), counts = unclass(ug))
})
class(unigramDT)
dim(unigramDT)
unigramDT[,1:10]
rm(unigramDT)
unigram_tau <- ngram(t1_dscpt$text, 1)
unigram_wf <- data.table(unigram = names(unigram_tau), counts = unclass(unigram_tau))
rm(unigram_tau)
hist(unigram_wf)
plot(unigram_wf)
barplot(unigram_wf)
names(unigram_wf)
barplot(counts ~ unigram, data = unigram_wf)
barplot(counts, data = unigram_wf)
barplot(unigram_wf$counts)
class(unigram_wf)
plot(unigram_wf$counts)
head(unigram_wf)
head(unigram_wf, 20)
head(unigram_wf, 50)
head(unigram_wf, 100)
head(unigram_wf, 10)
bigram_tau <- ngram(t1_dscpt$text, 1)
bigram_wf <- data.table(bigram = names(bigram_tau), counts = unclass(bigram_tau))
rm(bigram_tau)
head(bigram_wf, 10)
bigram_tau <- ngram(t1_dscpt$text, 2)
bigram_wf <- data.table(bigram = names(bigram_tau), counts = unclass(bigram_tau))
rm(bigram_tau)
head(bigram_wf, 10)
head(bigram_wf, 20)
head(bigram_wf, 40)
head(bigram_wf, 100)
trigram_tau <- ngram(t1_dscpt$text, 3)
trigram_wf <- data.table(trigram = names(trigram_tau), counts = unclass(trigram_tau))
rm(trigram_tau)
head(trigram_wf, 10)
head(trigram_wf, 20)
library(tm)
description <- t1$description
packages <- c("jsonlite", "dplyr", "data.table", "purrr")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load train
t1 <- fromJSON("../input/train.json")
vars <- setdiff(names(t1), c("photos", "features")) # set operation: difference
t1 <- map_at(t1, vars, unlist) %>% as.data.table(.)
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, removeWords, c("kaggle", "sigma", "renthop"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# transfer cleaned corpus to vector
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
library(tau)
# custom function to create N-Grams from vector texts
ngram <- function(docs, n) {textcnt(docs, method = "string",n=as.integer(n),
split = "[ ]+",decreasing=T)}
trigram_tau <- ngram(t1_dscpt$text, 3)
trigram_wf <- data.table(trigram = names(trigram_tau), counts = unclass(trigram_tau))
rm(trigram_tau)
head(trigram_wf, 20)
description <- t1$description
grepl("kaggle", description)[1:10]
head(description[grepl("kaggle", description)], 20)
gsub("(!|.).*kagglemanager@renthop.com.*", "", "Must check out this Large Studio Lots of sun and great closet space!  Spacious Kitchen with stainless steel appliances, and dishwasher. Marble bathrooms, with sliding glassed doors in bathtub. Its is an elevator building with Laundry and live in superintendent. It is in a great location, close to various subways, schools, good bars and restaurants , shopping and so much more.<br /><br />For more information regarding this Apartment feel free to contact me Fitim at (057-048-2286 or email me at kagglemanager@renthop.com<br /><br /><br /><br /><br /><br /><br /><br /><p><a  website_redacted")
gsub("(!|.)(.*)kagglemanager@renthop.com(.*)", "", "Must check out this Large Studio Lots of sun and great closet space!  Spacious Kitchen with stainless steel appliances, and dishwasher. Marble bathrooms, with sliding glassed doors in bathtub. Its is an elevator building with Laundry and live in superintendent. It is in a great location, close to various subways, schools, good bars and restaurants , shopping and so much more.<br /><br />For more information regarding this Apartment feel free to contact me Fitim at (057-048-2286 or email me at kagglemanager@renthop.com<br /><br /><br /><br /><br /><br /><br /><br /><p><a  website_redacted")
gsub("[!.](.*)kagglemanager@renthop.com(.*)", "", "Must check out this Large Studio Lots of sun and great closet space!  Spacious Kitchen with stainless steel appliances, and dishwasher. Marble bathrooms, with sliding glassed doors in bathtub. Its is an elevator building with Laundry and live in superintendent. It is in a great location, close to various subways, schools, good bars and restaurants , shopping and so much more.<br /><br />For more information regarding this Apartment feel free to contact me Fitim at (057-048-2286 or email me at kagglemanager@renthop.com<br /><br /><br /><br /><br /><br /><br /><br /><p><a  website_redacted")
gsub("[!.]([^!.]*)kagglemanager@renthop.com([^!.]*)", "", "Must check out this Large Studio Lots of sun and great closet space!  Spacious Kitchen with stainless steel appliances, and dishwasher. Marble bathrooms, with sliding glassed doors in bathtub. Its is an elevator building with Laundry and live in superintendent. It is in a great location, close to various subways, schools, good bars and restaurants , shopping and so much more.<br /><br />For more information regarding this Apartment feel free to contact me Fitim at (057-048-2286 or email me at kagglemanager@renthop.com<br /><br /><br /><br /><br /><br /><br /><br /><p><a  website_redacted")
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)kagglemanager@renthop.com([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# transfer cleaned corpus to vector
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
library(tau)
# custom function to create N-Grams from vector texts
ngram <- function(docs, n) {textcnt(docs, method = "string",n=as.integer(n),
split = "[ ]+",decreasing=T)}
bigram_tau <- ngram(t1_dscpt$text, 2)
bigram_wf <- data.table(bigram = names(bigram_tau), counts = unclass(bigram_tau))
rm(bigram_tau)
head(bigram_wf, 10)
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)kagglemanager@renthop.com([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, toString, " ([^!.]*)website redacted([^!.]*)", "")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# transfer cleaned corpus to vector
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
bigram_tau <- ngram(t1_dscpt$text, 2)
bigram_wf <- data.table(bigram = names(bigram_tau), counts = unclass(bigram_tau))
rm(bigram_tau)
head(bigram_wf, 20)
trigram_tau <- ngram(t1_dscpt$text, 3)
trigram_wf <- data.table(trigram = names(trigram_tau), counts = unclass(trigram_tau))
rm(trigram_tau)
head(trigram_wf, 20)
qgram_tau <- ngram(t1_dscpt$text, 4)
qgram_wf <- data.table(quadgram = names(qgram_tau), counts = unclass(qgram_tau))
rm(qgram_tau)
head(qgram_wf, 20)
bgram_tau <- ngram(t1_dscpt$text, 2)
bgram_wf <- data.table(bigram = names(bgram_tau), counts = unclass(bgram_tau))
rm(bgram_tau)
head(bgram_wf, 20)
ugram_tau <- ngram(t1_dscpt$text, 2)
ugram_wf <- data.table(unigram = names(ugram_tau), counts = unclass(ugram_tau))
rm(ugram_tau)
head(ugram_wf, 20)
ugram_tau <- ngram(t1_dscpt$text, 1)
ugram_wf <- data.table(unigram = names(ugram_tau), counts = unclass(ugram_tau))
rm(ugram_tau)
head(ugram_wf, 20)
bgram_tau <- ngram(t1_dscpt$text, 2)
bgram_wf <- data.table(bigram = names(bgram_tau), counts = unclass(bgram_tau))
rm(bgram_tau)
head(bgram_wf, 50)
BigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
bgram_dtm <- DocumentTermMatrix(mycorpus, control = list(tokenize = BigramTokenizer))
bgram_dtm_sub <- bgram_dtm[, bgram_wf$bigram[1:50]]
bgram_dtm
summary(bgram_dtm)
bgram_dtm
bgram_dtm$i
bgram_dtm$j
bgram_dtm$
v
summary(bgram_dtm)
bgram_dtm
names(bgram_dtm)
bgram_dtm$dimnames
bgram_dtm
bgram_freq <- sort(rowSums(as.matrix(bgram_tdm)), decreasing = TRUE)
bgram_freq <- sort(rowSums(as.matrix(bgram_dtm)), decreasing = TRUE)
tail(bgram_dtm$dimnames$Terms)
bgram_dtm_sub <- bgram_dtm[,1:20]
bgram_freq <- sort(rowSums(as.matrix(bgram_tdm_sub)), decreasing = TRUE)
bgram_freq <- sort(rowSums(as.matrix(bgram_dtm_sub)), decreasing = TRUE)
bgram_freq
bigram.wf <- data.frame(words = names(bgram_freq), freq = bgram_freq,
row.names = NULL, stringsAsFactors = FALSE)
bigram_wf
bigram.wf
bgram_freq <- sort(colSums(as.matrix(bgram_dtm_sub)), decreasing = TRUE)
bigram.wf <- data.frame(words = names(bgram_freq), freq = bgram_freq,
row.names = NULL, stringsAsFactors = FALSE)
bigram.wf
head(bigram_wf, 20)
head(bgram_wf, 20)
head(ugram_wf, 20)
# load necessary packages
packages <- c("jsonlite", "dplyr", "data.table", "purrr")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load train
t1 <- fromJSON("../input/train.json")
vars <- setdiff(names(t1), c("photos", "features")) # set operation: difference
t1 <- map_at(t1, vars, unlist) %>% as.data.table(.)
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)kagglemanager@renthop.com([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, toString, " ([^!.]*)website redacted([^!.]*)", "")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# transfer cleaned corpus to vector
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
BigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
bgram_dtm <- DocumentTermMatrix(mycorpus, control = list(tokenize = BigramTokenizer))
bgram_dtm$dimnames$Terms[1:20]
mycorpus[1]
mycorpus[[1]]$content
mycorpus[[2]]$content
mycorpus[1][1]
mycorpus$`1`
mycorpus[1]
mycorpus[1][[1]]
mycorpus[[1]]$content
mycorpus[[2]]$content
mycorpus[[10]]$content
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
t1_dscpt$text[1]
t1_dscpt$text[2]
t1_dscpt$text[3]
t1_dscpt$text[4]
t1_dscpt$text[5]
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)kagglemanager@renthop.com([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
#mycorpus <- tm_map(mycorpus, toString, " ([^!.]*)website redacted([^!.]*)", "")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# transfer cleaned corpus to vector
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
t1_dscpt$text[1]
t1_dscpt$text[2]
t1_dscpt$text[3]
t1_dscpt$text[4]
t1_dscpt$text[5]
t1_dscpt$text[6]
t1_dscpt$text[7]
t1_dscpt$text[8]
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)kagglemanager@renthop.com([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)website redacted([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# transfer cleaned corpus to vector
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
t1_dscpt$text[1]
library(tm)
description <- t1$description
mycorpus <- Corpus(VectorSource(description))
rm(description)
# corpus cleaning
toString <- content_transformer(function(x, from, to) gsub(from, to, x))
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)kagglemanager@renthop.com([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "[!.]([^!.]*)website(.*)redacted([^!.]*)", "")
mycorpus <- tm_map(mycorpus, toString, "<.*>", " ")
mycorpus <- tm_map(mycorpus, toString, "[[:punct:]]", " ")
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, removeNumbers)
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
mycorpus <- tm_map(mycorpus, stemDocument)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, toString,
"( ([a-z]){1,3})+ |^([a-z]){1,3} | ([a-z]){1,3}$", " ")
mycorpus <- tm_map(mycorpus, stripWhitespace)
# transfer cleaned corpus to vector
t1_dscpt <- data.table(listing_id = t1$listing_id, text = get("content", mycorpus))
t1_dscpt$text[1]
BigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
bgram_dtm <- DocumentTermMatrix(mycorpus, control = list(tokenize = BigramTokenizer))
bgram_dtm$dimnames$Terms[1:20]
?ngrams
?words
words(mycorpus)
vmycorpus <- VCorpus(VectorSource(description))
description <- t1$description
vmycorpus <- VCorpus(VectorSource(description))
words(mycorpus)
words(vmycorpus)
words(mycorpus[[1]]$content)
ngrams(words(mycorpus[[1]]$content, 2))
ngrams(words(mycorpus[[1]]$content), 2)
lapply(ngrams(words(mycorpus[[1]]$content), 2), paste, collapse = " ")
?unlist
unlist(lapply(ngrams(words(mycorpus[[1]]$content), 2), paste, collapse = " "))
unlist(lapply(ngrams(words(mycorpus[[1]]$content), 2), paste, collapse = " "), use.names = FALSE)
DocumentTermMatrix(mycorpus[[1]], control = list(tokenize = BigramTokenizer))
DocumentTermMatrix(mycorpus[1], control = list(tokenize = BigramTokenizer))
wf <- DocumentTermMatrix(mycorpus[1], control = list(tokenize = BigramTokenizer))
as.matrix(wf)
freq <- sort(colSums(as.matrix(tdm)), decreasing = TRUE)
wf <- data.frame(words = names(freq), freq = freq,
row.names = NULL, stringsAsFactors = FALSE)
dtm <- DocumentTermMatrix(mycorpus[1], control = list(tokenize = BigramTokenizer))
freq <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
wf <- data.frame(words = names(freq), freq = freq,
row.names = NULL, stringsAsFactors = FALSE)
wf
?DocumentTermMatrix
BigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = "_"), use.names = FALSE)
dtm <- DocumentTermMatrix(mycorpus[1], control = list(tokenize = BigramTokenizer))
freq <- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
wf <- data.frame(words = names(freq), freq = freq,
row.names = NULL, stringsAsFactors = FALSE)
wf
dtm$dimnames$Terms
library("tm")
data("crude")
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(removeSparseTerms(tdm[, 1:10], 0.7))
tdm$dimnames$Terms
library("tm")
data("crude")
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dtm <- DocumentTermMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(removeSparseTerms(dtm[1:10,], 0.7))
dtm$dimnames$Terms
tdm$dimnames$Terms
?removeSparseTerms
library("tm")
data("crude")
BigramTokenizer <-
function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dtm <- DocumentTermMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(removeSparseTerms(dtm[1:10,], 0.7))
dtm$dimnames$Terms
BigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
bgram_dtm <- TermDocumentMatrix(mycorpus, control = list(tokenize = BigramTokenizer))
bgram_dtm <- removeSparseTerms(bgram_dtm, 0.8)
inspect(bgram_dtm)
BigramTokenizer <- function(x)
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
bgram_dtm <- DocumentTermMatrix(mycorpus[1], control = list(tokenize = BigramTokenizer))
bgram_dtm <- removeSparseTerms(bgram_dtm, 0.8)
inspect(bgram_dtm)
q()
q()
